{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Download**"
      ],
      "metadata": {
        "id": "IFzRp5_pdWG8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaVLTWj2767r",
        "outputId": "33c76a55-c6b8-4418-954e-328d7bca47ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oASNDwimfBI1a0PfKb2CwBd4ajLlM15J\n",
            "To: /content/dataloader_demo.ipynb\n",
            "100% 5.79k/5.79k [00:00<00:00, 17.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Zfb0UFlqX1XyTXA8eQmNGNjxChL2jeT3\n",
            "To: /content/exploring_data.py\n",
            "100% 3.27k/3.27k [00:00<00:00, 15.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=195P4Hb5H_JZkVADsz3mj4MJZ18NMHeMX\n",
            "To: /content/triplet_loss.py\n",
            "100% 2.88k/2.88k [00:00<00:00, 15.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Srx0e8Nr08rJTgXRgkWlz_jEqiF8qry6\n",
            "To: /content/utils.py\n",
            "100% 5.76k/5.76k [00:00<00:00, 20.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XhJCD8MXNF8QPyn9k5nX7LL2maJrJoax\n",
            "To: /content/12000_test_mnistmnistmsvhnsynusps.npz\n",
            "100% 70.6M/70.6M [00:00<00:00, 113MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1L4NdvtvnOGEsHQW8Hh2tV_3-Awhu_rod\n",
            "To: /content/12000_train_mnistmnistmsvhnsynusps.npz\n",
            "100% 197M/197M [00:01<00:00, 111MB/s] \n"
          ]
        }
      ],
      "source": [
        "!gdown 1oASNDwimfBI1a0PfKb2CwBd4ajLlM15J\n",
        "!gdown 1Zfb0UFlqX1XyTXA8eQmNGNjxChL2jeT3\n",
        "!gdown 195P4Hb5H_JZkVADsz3mj4MJZ18NMHeMX\n",
        "!gdown 1Srx0e8Nr08rJTgXRgkWlz_jEqiF8qry6\n",
        "!gdown 1XhJCD8MXNF8QPyn9k5nX7LL2maJrJoax\n",
        "!gdown 1L4NdvtvnOGEsHQW8Hh2tV_3-Awhu_rod"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import**"
      ],
      "metadata": {
        "id": "PZIdLb2OdcEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import get_data_loaders\n",
        "from triplet_loss import triplet_loss\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "EbDc_oquG_nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Method**"
      ],
      "metadata": {
        "id": "S9CGlq2DdiP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loaders_demo():\n",
        "    full_dataloaders, _ = get_data_loaders(\n",
        "        filenames={\n",
        "            'train': '12000_train_mnistmnistmsvhnsynusps.npz',\n",
        "            'test': '12000_test_mnistmnistmsvhnsynusps.npz',\n",
        "        },\n",
        "        batch_size= 64\n",
        "    )\n",
        "    print(full_dataloaders.keys())\n",
        "\n",
        "    for phase in ['train', 'test', 'test_missing']:\n",
        "        print(f'{phase} data size: ', full_dataloaders[f'{phase}_size'])\n",
        "        for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(full_dataloaders[phase]):\n",
        "            print(f'{batch_indx}-th batch')\n",
        "            print('images shape: ', images.shape)\n",
        "            print('features shape: ', features.shape)\n",
        "            if phase == 'test_missing':\n",
        "                print('in test-missing dataloaders, since the features are not available, features are filled with zeros', torch.sum(features))\n",
        "            print('domain labels freq: ', torch.unique(domain_labels, return_counts=True))\n",
        "            print('digit labels freq: ', torch.unique(digit_labels, return_counts=True))\n",
        "            print()\n",
        "            break\n",
        "loaders_demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CICUYkL1Gb9Q",
        "outputId": "eea72701-e6c2-4b7f-ad32-01261284eefa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datafiles to read:  {'train': '12000_train_mnistmnistmsvhnsynusps.npz', 'test': '12000_test_mnistmnistmsvhnsynusps.npz'}\n",
            "reading 12000_train_mnistmnistmsvhnsynusps.npz, number of samples: 60000\n",
            "reading 12000_test_mnistmnistmsvhnsynusps.npz, number of samples: 21600\n",
            "reading 12000_test_mnistmnistmsvhnsynusps.npz, number of samples: 21600\n",
            "dict_keys(['train', 'test', 'test_missing', 'train_size', 'test_size', 'test_missing_size'])\n",
            "train data size:  60000\n",
            "0-th batch\n",
            "images shape:  torch.Size([64, 3, 32, 32])\n",
            "features shape:  torch.Size([64, 256])\n",
            "domain labels freq:  (tensor([0, 1, 2, 3, 4]), tensor([10, 13, 11, 18, 12]))\n",
            "digit labels freq:  (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([ 6,  5,  8,  5,  6,  9, 10,  7,  5,  3]))\n",
            "\n",
            "test data size:  21600\n",
            "0-th batch\n",
            "images shape:  torch.Size([64, 3, 32, 32])\n",
            "features shape:  torch.Size([64, 256])\n",
            "domain labels freq:  (tensor([0, 1, 2, 3, 4]), tensor([ 9,  9, 17, 15, 14]))\n",
            "digit labels freq:  (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([ 5, 12,  6,  2,  2,  8,  6,  5,  9,  9]))\n",
            "\n",
            "test_missing data size:  21600\n",
            "0-th batch\n",
            "images shape:  torch.Size([64, 3, 32, 32])\n",
            "features shape:  torch.Size([64, 256])\n",
            "in test-missing dataloaders, since the features are not available, features are filled with zeros tensor(0.)\n",
            "domain labels freq:  (tensor([0, 1, 2, 3, 4]), tensor([17,  8, 12, 13, 14]))\n",
            "digit labels freq:  (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([ 4,  8,  8,  3,  9, 12,  7,  4,  5,  4]))\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loading**"
      ],
      "metadata": {
        "id": "Kx2q50kKdl9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataloaders, _ = get_data_loaders(\n",
        "        filenames={\n",
        "            'train': '12000_train_mnistmnistmsvhnsynusps.npz',\n",
        "            'test': '12000_test_mnistmnistmsvhnsynusps.npz',\n",
        "        },\n",
        "        batch_size= 64\n",
        "    )\n",
        "train = full_dataloaders['train']\n",
        "test = full_dataloaders['test']\n",
        "missing = full_dataloaders['test_missing']\n",
        "# for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(train):\n",
        "#             print(images.shape)\n",
        "#             break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a73PTUJGLAxW",
        "outputId": "ed3e3ead-823a-4408-e0f0-bd0375157624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datafiles to read:  {'train': '12000_train_mnistmnistmsvhnsynusps.npz', 'test': '12000_test_mnistmnistmsvhnsynusps.npz'}\n",
            "reading 12000_train_mnistmnistmsvhnsynusps.npz, number of samples: 60000\n",
            "reading 12000_test_mnistmnistmsvhnsynusps.npz, number of samples: 21600\n",
            "reading 12000_test_mnistmnistmsvhnsynusps.npz, number of samples: 21600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model (First Try)**"
      ],
      "metadata": {
        "id": "2j5Z-VxndpRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedModel2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc_img = nn.Linear(32 * 32 * 32, 256)\n",
        "        self.fc_combined = nn.Linear(512, 256)# Concatenated feature and image size\n",
        "        self.fc_combined2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x_img, x_features):\n",
        "        x_img = self.conv1(x_img)\n",
        "        x_img = self.conv2(x_img)\n",
        "        x_img = x_img.view(x_img.size(0), -1)  # Flatten the image\n",
        "        x_img = self.fc_img(x_img)\n",
        "\n",
        "        # Concatenate features and image representations\n",
        "        x_combined = torch.cat((x_img, x_features), dim=1)\n",
        "\n",
        "        # Fully connected layers for prediction\n",
        "        x_combined = self.fc_combined(x_combined)\n",
        "        x_combined2 = self.fc_combined2(x_combined)\n",
        "        return x_combined2"
      ],
      "metadata": {
        "id": "m8IKmfBOkA4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# margin: 0.005 and lambda: 0\n",
        "a = 0\n",
        "model = CombinedModel2()\n",
        "crossLoss = nn.CrossEntropyLoss()\n",
        "triplet = triplet_loss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  running_corrects = 0\n",
        "  running_loss = 0.0\n",
        "  num_samples = len(train.dataset)\n",
        "  num_batches = len(train)\n",
        "  for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(train):\n",
        "    outs = model(images, features)\n",
        "    loss1 = crossLoss(outs, digit_labels)\n",
        "\n",
        "    # this loss is for domain labels (update later...)\n",
        "    # loss2 = triplet(outs, digit_labels)\n",
        "\n",
        "    # total_loss = loss1 + a * loss2\n",
        "\n",
        "    loss1.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    _, preds = torch.max(outs, dim=1) # Explain, [N]\n",
        "    running_corrects += torch.sum(preds == digit_labels)\n",
        "    running_loss += loss1.item()\n",
        "\n",
        "\n",
        "  epoch_acc = (running_corrects / num_samples) * 100\n",
        "  epoch_loss = (running_loss / num_batches)\n",
        "  print(epoch_acc)\n",
        "  print(epoch_loss)\n",
        "\n",
        "\n",
        "  num_samples_test = len(test.dataset)\n",
        "  num_batches_test = len(test)\n",
        "  running_corrects_test = 0\n",
        "  running_loss_test = 0.0\n",
        "\n",
        "\n",
        "  model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "  with torch.no_grad(): # explain\n",
        "      # more on torch.no_grad(): https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#disabling-gradient-tracking\n",
        "\n",
        "      for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(test): # Get a batch of Data\n",
        "\n",
        "          outputs = model(images, features) # Forward Pass\n",
        "          loss = crossLoss(outputs, digit_labels) # Compute Loss\n",
        "\n",
        "          # loss.backward() # Compute Gradients\n",
        "          # optim.step() # Update parameters\n",
        "          # optim.zero_grad() # zero the parameter's gradients\n",
        "\n",
        "          _, preds = torch.max(outputs, 1) #\n",
        "          running_corrects_test += torch.sum(preds == digit_labels)\n",
        "          running_loss_test += loss.item()\n",
        "\n",
        "  test_acc = (running_corrects_test / num_samples_test) * 100\n",
        "  test_loss = (running_loss_test / num_batches_test)\n",
        "  print(test_acc)\n",
        "  print(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46vtLlHFntzI",
        "outputId": "6d7605e5-72fe-4aa3-c120-8e5f3647da1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(47.2983)\n",
            "1.7020510615570459\n",
            "tensor(51.3565)\n",
            "1.5375995064628194\n",
            "tensor(59.5717)\n",
            "1.2787628383524636\n",
            "tensor(69.1898)\n",
            "0.9718344797044111\n",
            "tensor(76.1600)\n",
            "0.7707249532693993\n",
            "tensor(80.0463)\n",
            "0.658575889362386\n",
            "tensor(81.9283)\n",
            "0.5900874832577543\n",
            "tensor(83.2870)\n",
            "0.5708600082076513\n",
            "tensor(84.4167)\n",
            "0.5187758084839341\n",
            "tensor(83.4259)\n",
            "0.5617636193978716\n",
            "tensor(85.4400)\n",
            "0.48417612149326533\n",
            "tensor(84.0787)\n",
            "0.5432060827341305\n",
            "tensor(86.2383)\n",
            "0.4578201708985544\n",
            "tensor(84.2685)\n",
            "0.5401536630543732\n",
            "tensor(86.9200)\n",
            "0.43417919178538994\n",
            "tensor(84.6991)\n",
            "0.5265429215671042\n",
            "tensor(87.4550)\n",
            "0.4175378336453997\n",
            "tensor(84.8380)\n",
            "0.5301689260016532\n",
            "tensor(88.0433)\n",
            "0.39747335602130207\n",
            "tensor(85.7269)\n",
            "0.5028339879664444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.fc_img = nn.Linear(32 * 8 * 8, 256)\n",
        "        self.fc_combined = nn.Linear(512, 256)# Concatenated feature and image size\n",
        "        self.fc_combined2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x_img, x_features):\n",
        "        x_img = self.conv1(x_img)\n",
        "        x_img = self.conv2(x_img)\n",
        "        x_img = x_img.view(x_img.size(0), -1)  # Flatten the image\n",
        "        x_img = self.fc_img(x_img)\n",
        "\n",
        "        # Concatenate features and image representations\n",
        "        x_combined = torch.cat((x_img, x_features), dim=1)\n",
        "\n",
        "        # Fully connected layers for prediction\n",
        "        x_combined = self.fc_combined(x_combined)\n",
        "        x_combined2 = self.fc_combined2(x_combined)\n",
        "        return x_combined2, x_combined"
      ],
      "metadata": {
        "id": "58zv1Hzka0Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# margin: 0.005 and lambda: 0.2\n",
        "a = 0.2\n",
        "model = CombinedModel()\n",
        "crossLoss = nn.CrossEntropyLoss()\n",
        "triplet = triplet_loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  print('epoch: ' + str(epoch))\n",
        "  model.train()\n",
        "  running_corrects = 0\n",
        "  running_loss = 0.0\n",
        "  num_samples = len(train.dataset)\n",
        "  num_batches = len(train)\n",
        "  for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(train):\n",
        "\n",
        "    outs2, outs = model(images, features)\n",
        "\n",
        "    loss1 = crossLoss(outs2, digit_labels)\n",
        "    loss2 = triplet(outs, domain_labels)\n",
        "    total_loss = loss1 + a * loss2\n",
        "\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    _, preds = torch.max(outs2, dim=1) # Explain, [N]\n",
        "    running_corrects += torch.sum(preds == digit_labels)\n",
        "    running_loss += total_loss.item()\n",
        "\n",
        "  epoch_acc = (running_corrects / num_samples) * 100\n",
        "  epoch_loss = (running_loss / num_batches)\n",
        "  print(epoch_acc)\n",
        "  print(epoch_loss)\n",
        "\n",
        "\n",
        "  num_samples_test = len(test.dataset)\n",
        "  num_batches_test = len(test)\n",
        "  running_corrects_test = 0\n",
        "  running_loss_test = 0.0\n",
        "\n",
        "\n",
        "  model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "  with torch.no_grad(): # explain\n",
        "      # more on torch.no_grad(): https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#disabling-gradient-tracking\n",
        "\n",
        "      for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(test): # Get a batch of Data\n",
        "\n",
        "          outputs2, outputs = model(images, features) # Forward Pass\n",
        "          loss = crossLoss(outputs2, digit_labels) # Compute Loss\n",
        "          loss2 = triplet(outputs, domain_labels)\n",
        "          total_loss = loss + a * loss2\n",
        "\n",
        "          _, preds = torch.max(outputs2, 1) #\n",
        "          running_corrects_test += torch.sum(preds == digit_labels)\n",
        "          running_loss_test += total_loss.item()\n",
        "\n",
        "  test_acc = (running_corrects_test / num_samples_test) * 100\n",
        "  test_loss = (running_loss_test / num_batches_test)\n",
        "  print(test_acc)\n",
        "  print(test_loss)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd6b70bUYpIO",
        "outputId": "10709a87-e408-4dc9-cd98-b046063d4faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "tensor(78.1767)\n",
            "1.0990127398451763\n",
            "tensor(84.3657)\n",
            "1.1633740772862406\n",
            "\n",
            "epoch: 1\n",
            "tensor(87.2500)\n",
            "0.6564978392266515\n",
            "tensor(87.0787)\n",
            "1.4364582007453286\n",
            "\n",
            "epoch: 2\n",
            "tensor(89.0567)\n",
            "0.5723801645071014\n",
            "tensor(87.2963)\n",
            "1.3237379376704876\n",
            "\n",
            "epoch: 3\n",
            "tensor(90.0050)\n",
            "0.5190424712767987\n",
            "tensor(88.1713)\n",
            "1.1418375200068458\n",
            "\n",
            "epoch: 4\n",
            "tensor(90.7500)\n",
            "0.4819496440321906\n",
            "tensor(88.2083)\n",
            "1.248982205898804\n",
            "\n",
            "epoch: 5\n",
            "tensor(91.1933)\n",
            "0.4553994424879424\n",
            "tensor(88.6111)\n",
            "1.1107811735579247\n",
            "\n",
            "epoch: 6\n",
            "tensor(91.9600)\n",
            "0.4255849871633531\n",
            "tensor(88.9352)\n",
            "1.0354671809800278\n",
            "\n",
            "epoch: 7\n",
            "tensor(92.2033)\n",
            "0.4134324026514472\n",
            "tensor(88.6713)\n",
            "1.1672119134981958\n",
            "\n",
            "epoch: 8\n",
            "tensor(92.4467)\n",
            "0.3974411038161595\n",
            "tensor(87.9537)\n",
            "1.158520528197994\n",
            "\n",
            "epoch: 9\n",
            "tensor(92.8867)\n",
            "0.3789314676258864\n",
            "tensor(89.3241)\n",
            "1.2218802289144528\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# margin: 0.05 and lambda: 0.15\n",
        "a = 0.15\n",
        "model = CombinedModel()\n",
        "crossLoss = nn.CrossEntropyLoss()\n",
        "triplet = triplet_loss(margin=0.05)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  print('epoch: ' + str(epoch))\n",
        "  model.train()\n",
        "  running_corrects = 0\n",
        "  running_loss = 0.0\n",
        "  num_samples = len(train.dataset)\n",
        "  num_batches = len(train)\n",
        "  for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(train):\n",
        "\n",
        "    outs2, outs = model(images, features)\n",
        "\n",
        "    loss1 = crossLoss(outs2, digit_labels)\n",
        "    loss2 = triplet(outs, domain_labels)\n",
        "    total_loss = loss1 + a * loss2\n",
        "\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    _, preds = torch.max(outs2, dim=1) # Explain, [N]\n",
        "    running_corrects += torch.sum(preds == digit_labels)\n",
        "    running_loss += total_loss.item()\n",
        "\n",
        "  epoch_acc = (running_corrects / num_samples) * 100\n",
        "  epoch_loss = (running_loss / num_batches)\n",
        "  print(epoch_acc)\n",
        "  print(epoch_loss)\n",
        "\n",
        "\n",
        "  num_samples_test = len(test.dataset)\n",
        "  num_batches_test = len(test)\n",
        "  running_corrects_test = 0\n",
        "  running_loss_test = 0.0\n",
        "\n",
        "\n",
        "  model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "  with torch.no_grad(): # explain\n",
        "      # more on torch.no_grad(): https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#disabling-gradient-tracking\n",
        "\n",
        "      for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(test): # Get a batch of Data\n",
        "\n",
        "          outputs2, outputs = model(images, features) # Forward Pass\n",
        "          loss = crossLoss(outputs2, digit_labels) # Compute Loss\n",
        "          loss2 = triplet(outputs, domain_labels)\n",
        "          total_loss = loss + a * loss2\n",
        "\n",
        "          _, preds = torch.max(outputs2, 1) #\n",
        "          running_corrects_test += torch.sum(preds == digit_labels)\n",
        "          running_loss_test += total_loss.item()\n",
        "\n",
        "  test_acc = (running_corrects_test / num_samples_test) * 100\n",
        "  test_loss = (running_loss_test / num_batches_test)\n",
        "  print(test_acc)\n",
        "  print(test_loss)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_nv9kwwjL0G",
        "outputId": "7b861221-ba32-4657-cd42-cf62b1f17e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "tensor(76.5350)\n",
            "1.0565632909282183\n",
            "tensor(84.5648)\n",
            "0.7265627738639447\n",
            "\n",
            "epoch: 1\n",
            "tensor(86.3867)\n",
            "0.5956343043206344\n",
            "tensor(86.4074)\n",
            "0.6579723594456734\n",
            "\n",
            "epoch: 2\n",
            "tensor(87.9933)\n",
            "0.5027762494965403\n",
            "tensor(85.6620)\n",
            "0.7908591673924372\n",
            "\n",
            "epoch: 3\n",
            "tensor(89.2117)\n",
            "0.45318882272981886\n",
            "tensor(86.9769)\n",
            "0.6175758858933251\n",
            "\n",
            "epoch: 4\n",
            "tensor(90.1533)\n",
            "0.41575332109862045\n",
            "tensor(87.1481)\n",
            "0.605746651013222\n",
            "\n",
            "epoch: 5\n",
            "tensor(90.4733)\n",
            "0.39169425989137785\n",
            "tensor(88.2685)\n",
            "0.5877323985628827\n",
            "\n",
            "epoch: 6\n",
            "tensor(91.2217)\n",
            "0.3679653053312922\n",
            "tensor(87.7870)\n",
            "0.607917448869471\n",
            "\n",
            "epoch: 7\n",
            "tensor(91.6717)\n",
            "0.3472968633574591\n",
            "tensor(87.8333)\n",
            "0.6373053320501683\n",
            "\n",
            "epoch: 8\n",
            "tensor(91.9767)\n",
            "0.33660663800167123\n",
            "tensor(88.3704)\n",
            "0.5811972328897058\n",
            "\n",
            "epoch: 9\n",
            "tensor(92.3667)\n",
            "0.31619173244658566\n",
            "tensor(87.4630)\n",
            "0.6191634790608164\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fit Function**"
      ],
      "metadata": {
        "id": "dfFP41jgdz47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(margin, lambd):\n",
        "  model = CombinedModel()\n",
        "  crossLoss = nn.CrossEntropyLoss()\n",
        "  triplet = triplet_loss(margin=margin)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  epochs = 10\n",
        "  for epoch in range(epochs):\n",
        "    print('epoch: ' + str(epoch))\n",
        "    model.train()\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    triplet_total = 0.0\n",
        "    entropy_total = 0.0\n",
        "\n",
        "    num_samples = len(train.dataset)\n",
        "    num_batches = len(train)\n",
        "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(train):\n",
        "\n",
        "      outs2, outs = model(images, features)\n",
        "\n",
        "      loss1 = crossLoss(outs2, digit_labels)\n",
        "      loss2 = triplet(outs, domain_labels)\n",
        "      total_loss = loss1 + lambd * loss2\n",
        "\n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      _, preds = torch.max(outs2, dim=1) # Explain, [N]\n",
        "      running_corrects += torch.sum(preds == digit_labels)\n",
        "      running_loss += total_loss.item()\n",
        "      triplet_total += loss2.item()\n",
        "      entropy_total += loss1.item()\n",
        "\n",
        "    epoch_acc = (running_corrects / num_samples) * 100\n",
        "    epoch_loss = (running_loss / num_batches)\n",
        "    triplet_total  = (triplet_total / num_batches)\n",
        "    entropy_total = (entropy_total / num_batches)\n",
        "    print('train')\n",
        "    print('accuaray: ' + str(epoch_acc))\n",
        "    print('total loss: ' + str(epoch_loss))\n",
        "    print('triplet loss: ' + str(loss2))\n",
        "    print('cross entropy loss: ' + str(loss1))\n",
        "    print('---------------')\n",
        "\n",
        "\n",
        "    num_samples_test = len(test.dataset)\n",
        "    num_batches_test = len(test)\n",
        "    running_corrects_test = 0\n",
        "    running_loss_test = 0.0\n",
        "    triplet_total = 0.0\n",
        "    entropy_total = 0.0\n",
        "\n",
        "\n",
        "    model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "    with torch.no_grad(): # explain\n",
        "        # more on torch.no_grad(): https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#disabling-gradient-tracking\n",
        "\n",
        "        for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(test): # Get a batch of Data\n",
        "\n",
        "            outputs2, outputs = model(images, features) # Forward Pass\n",
        "            loss = crossLoss(outputs2, digit_labels) # Compute Loss\n",
        "            loss2 = triplet(outputs, domain_labels)\n",
        "            total_loss = loss + lambd * loss2\n",
        "\n",
        "            _, preds = torch.max(outputs2, 1) #\n",
        "            running_corrects_test += torch.sum(preds == digit_labels)\n",
        "            running_loss_test += total_loss.item()\n",
        "            triplet_total += loss2.item()\n",
        "            entropy_total += loss.item()\n",
        "\n",
        "    test_acc = (running_corrects_test / num_samples_test) * 100\n",
        "    test_loss = (running_loss_test / num_batches_test)\n",
        "    triplet_total = (triplet_total / num_batches_test)\n",
        "    entropy_total = (entropy_total / num_batches_test)\n",
        "    print('test')\n",
        "    print('accuray: ' + str(test_acc))\n",
        "    print('total loss: ' + str(test_loss))\n",
        "    print('triplet loss: ' + str(triplet_total))\n",
        "    print('cross entropy loss: ' + str(entropy_total))"
      ],
      "metadata": {
        "id": "U9KL44fLnc7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in [0.001, 0.005, 0.025]:\n",
        "  for l in [0.1, 0.2, 0.3]:\n",
        "    print('margin: ' + str(m) + ' lambda: ' + str(l))\n",
        "    fit(margin=m, lambd=l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxROFqvtpyxt",
        "outputId": "a6b685ac-fa04-48be-c35d-e5ed22803f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "margin: 0.001 lambda: 0.1\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(77.3883)\n",
            "total loss: 0.9857330186280615\n",
            "triplet loss: tensor(2.2516, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.8198, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(85.2500)\n",
            "total loss: 0.6607284118261563\n",
            "triplet loss: 1.7836046001967594\n",
            "cross entropy loss: 0.48236794990195325\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(86.6767)\n",
            "total loss: 0.5646098479628563\n",
            "triplet loss: tensor(0.8967, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3350, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.7870)\n",
            "total loss: 0.6138867261494405\n",
            "triplet loss: 1.8040978975549957\n",
            "cross entropy loss: 0.43347693195004433\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(88.4467)\n",
            "total loss: 0.47446373734932973\n",
            "triplet loss: tensor(0.9114, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2998, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.0463)\n",
            "total loss: 0.5737707739221979\n",
            "triplet loss: 1.492296055730807\n",
            "cross entropy loss: 0.42454116363849864\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(89.5567)\n",
            "total loss: 0.41956619168522513\n",
            "triplet loss: tensor(0.2193, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2658, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.5278)\n",
            "total loss: 0.5605403090105254\n",
            "triplet loss: 1.3927053264590235\n",
            "cross entropy loss: 0.42126977430469187\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(90.5367)\n",
            "total loss: 0.37956935886952925\n",
            "triplet loss: tensor(0.3815, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1295, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.7130)\n",
            "total loss: 0.5454441883860255\n",
            "triplet loss: 1.6361059671234803\n",
            "cross entropy loss: 0.3818335887154884\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(90.9050)\n",
            "total loss: 0.3590231446473837\n",
            "triplet loss: tensor(0.6700, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1565, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.0324)\n",
            "total loss: 0.6826226658810525\n",
            "triplet loss: 2.4273860690861766\n",
            "cross entropy loss: 0.43988405495397087\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(91.5067)\n",
            "total loss: 0.3416152326648296\n",
            "triplet loss: tensor(0.3479, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1577, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.9491)\n",
            "total loss: 0.5403622817799184\n",
            "triplet loss: 1.306091976043432\n",
            "cross entropy loss: 0.40975308171390784\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(91.8933)\n",
            "total loss: 0.3204975968548484\n",
            "triplet loss: tensor(0.5603, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0743, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.8287)\n",
            "total loss: 0.5701636719633136\n",
            "triplet loss: 1.4560500833718382\n",
            "cross entropy loss: 0.4245586624071443\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(92.2900)\n",
            "total loss: 0.3026527110526938\n",
            "triplet loss: tensor(0.3229, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3387, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.9769)\n",
            "total loss: 0.5665293133558607\n",
            "triplet loss: 1.4718467195093985\n",
            "cross entropy loss: 0.4193446387553356\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(92.6933)\n",
            "total loss: 0.29247980938553175\n",
            "triplet loss: tensor(0., grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2126, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.2731)\n",
            "total loss: 0.555404332835646\n",
            "triplet loss: 1.3380917257205005\n",
            "cross entropy loss: 0.4215951571626776\n",
            "margin: 0.001 lambda: 0.2\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(75.0250)\n",
            "total loss: 1.1761610054575813\n",
            "triplet loss: tensor(0.2501, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.5070, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(83.4352)\n",
            "total loss: 0.7984208715032544\n",
            "triplet loss: 1.3336855047024214\n",
            "cross entropy loss: 0.5316837641614429\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(85.2433)\n",
            "total loss: 0.6431921127635533\n",
            "triplet loss: tensor(1.2106, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1569, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.1528)\n",
            "total loss: 0.677044368442699\n",
            "triplet loss: 1.1236950459741277\n",
            "cross entropy loss: 0.4523053543542969\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(87.3750)\n",
            "total loss: 0.538839202223302\n",
            "triplet loss: tensor(0.2388, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4462, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.1296)\n",
            "total loss: 0.6299091808982855\n",
            "triplet loss: 1.051609406809835\n",
            "cross entropy loss: 0.4195872950836046\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(88.6283)\n",
            "total loss: 0.48243470934789573\n",
            "triplet loss: tensor(0.1229, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4687, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.9815)\n",
            "total loss: 0.6577958425826574\n",
            "triplet loss: 1.1686052359949202\n",
            "cross entropy loss: 0.4240747919008577\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(89.8300)\n",
            "total loss: 0.431783986673045\n",
            "triplet loss: tensor(0.1597, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2627, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.2778)\n",
            "total loss: 0.6510034966221928\n",
            "triplet loss: 1.1081118996161416\n",
            "cross entropy loss: 0.42938111191229705\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(90.1950)\n",
            "total loss: 0.41085715640360104\n",
            "triplet loss: tensor(0.1121, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1710, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.1759)\n",
            "total loss: 0.6316452842845014\n",
            "triplet loss: 0.9752940542268507\n",
            "cross entropy loss: 0.4365864706021794\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(90.6900)\n",
            "total loss: 0.3921056129435486\n",
            "triplet loss: tensor(0.1497, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1985, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.7407)\n",
            "total loss: 0.6372512651265726\n",
            "triplet loss: 1.0939013793425272\n",
            "cross entropy loss: 0.4184709862976737\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(91.3267)\n",
            "total loss: 0.36810619957538554\n",
            "triplet loss: tensor(0., grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1076, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.5231)\n",
            "total loss: 0.6211735727871663\n",
            "triplet loss: 1.0047095202614922\n",
            "cross entropy loss: 0.4202316660191533\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(91.5833)\n",
            "total loss: 0.3582052963136483\n",
            "triplet loss: tensor(0.0177, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3935, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.0463)\n",
            "total loss: 0.623331627329073\n",
            "triplet loss: 1.0535976996882295\n",
            "cross entropy loss: 0.41261208429932594\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(91.9667)\n",
            "total loss: 0.33614671188218\n",
            "triplet loss: tensor(0.0048, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1660, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.8796)\n",
            "total loss: 0.6656903891697438\n",
            "triplet loss: 1.1970047238089982\n",
            "cross entropy loss: 0.4262894408986766\n",
            "margin: 0.001 lambda: 0.3\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(73.0600)\n",
            "total loss: 1.2938861514268907\n",
            "triplet loss: tensor(0.5916, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3593, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(83.0139)\n",
            "total loss: 0.8977361004028095\n",
            "triplet loss: 1.1700234768041493\n",
            "cross entropy loss: 0.5467290440078317\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(85.3017)\n",
            "total loss: 0.6708567629554378\n",
            "triplet loss: tensor(0.0400, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2514, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(85.4398)\n",
            "total loss: 0.7331891724696527\n",
            "triplet loss: 0.8891357038412574\n",
            "cross entropy loss: 0.46644845340378893\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(87.4333)\n",
            "total loss: 0.5653857340785995\n",
            "triplet loss: tensor(0.7182, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2633, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.2454)\n",
            "total loss: 0.7339806653691466\n",
            "triplet loss: 0.9661219046732201\n",
            "cross entropy loss: 0.4441440814345546\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(88.2883)\n",
            "total loss: 0.5104650541155069\n",
            "triplet loss: tensor(0.6820, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2857, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.4769)\n",
            "total loss: 0.6923151080601314\n",
            "triplet loss: 0.8155413320034566\n",
            "cross entropy loss: 0.44765269725456747\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(89.3683)\n",
            "total loss: 0.46496221107015734\n",
            "triplet loss: tensor(0.4705, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4977, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.6019)\n",
            "total loss: 0.6903792461876334\n",
            "triplet loss: 0.797631125797975\n",
            "cross entropy loss: 0.4510898985100921\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(89.9767)\n",
            "total loss: 0.4342243291262879\n",
            "triplet loss: tensor(0.0356, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.5414, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.0231)\n",
            "total loss: 0.7345604366187514\n",
            "triplet loss: 0.9805410972592481\n",
            "cross entropy loss: 0.4403980963769749\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(90.1417)\n",
            "total loss: 0.41815571176376676\n",
            "triplet loss: tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1884, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.7361)\n",
            "total loss: 0.7176743760793167\n",
            "triplet loss: 0.9753137413390289\n",
            "cross entropy loss: 0.42508024025774566\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(91.)\n",
            "total loss: 0.38927280241206513\n",
            "triplet loss: tensor(0.2533, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4243, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.0741)\n",
            "total loss: 0.7675547995687236\n",
            "triplet loss: 1.0797518483258388\n",
            "cross entropy loss: 0.4436292330012519\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(91.3900)\n",
            "total loss: 0.3698263741584856\n",
            "triplet loss: tensor(0.0451, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2976, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.4259)\n",
            "total loss: 0.7117221338685448\n",
            "triplet loss: 0.9262723211066787\n",
            "cross entropy loss: 0.4338404276579089\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(91.5467)\n",
            "total loss: 0.3607908163283235\n",
            "triplet loss: tensor(0., grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3871, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.7917)\n",
            "total loss: 0.6963795830424016\n",
            "triplet loss: 0.8787310053980165\n",
            "cross entropy loss: 0.4327602706100108\n",
            "margin: 0.005 lambda: 0.1\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(76.3550)\n",
            "total loss: 1.0156199358610203\n",
            "triplet loss: tensor(2.0944, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.6616, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(83.5602)\n",
            "total loss: 0.7360172647343585\n",
            "triplet loss: 2.054539277885087\n",
            "cross entropy loss: 0.5305633318318418\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(86.3267)\n",
            "total loss: 0.5818104610196563\n",
            "triplet loss: tensor(0.5860, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.6822, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.1204)\n",
            "total loss: 0.6218033036184029\n",
            "triplet loss: 1.725458022581755\n",
            "cross entropy loss: 0.4492574965239629\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(88.1783)\n",
            "total loss: 0.4883973564483972\n",
            "triplet loss: tensor(0.2883, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2424, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.3380)\n",
            "total loss: 0.548596852188985\n",
            "triplet loss: 1.3586787581443787\n",
            "cross entropy loss: 0.4127289741437816\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(89.3500)\n",
            "total loss: 0.427555942602122\n",
            "triplet loss: tensor(0.4062, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1642, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.1157)\n",
            "total loss: 0.551487845914251\n",
            "triplet loss: 1.2957628484837402\n",
            "cross entropy loss: 0.4219115598667303\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(90.2817)\n",
            "total loss: 0.38941626381804184\n",
            "triplet loss: tensor(0.0680, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2865, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.0880)\n",
            "total loss: 0.5403501456217653\n",
            "triplet loss: 1.4261637358160237\n",
            "cross entropy loss: 0.39773376804043553\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(90.6850)\n",
            "total loss: 0.36887623878048936\n",
            "triplet loss: tensor(1.1421, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3172, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.6343)\n",
            "total loss: 0.5451765898064044\n",
            "triplet loss: 1.3487600627184444\n",
            "cross entropy loss: 0.4103005817215118\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(91.4067)\n",
            "total loss: 0.3429424241105758\n",
            "triplet loss: tensor(0.8347, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3389, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.5463)\n",
            "total loss: 0.5280068980959746\n",
            "triplet loss: 1.3383721639595088\n",
            "cross entropy loss: 0.3941696793193648\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(91.9083)\n",
            "total loss: 0.32316359976080183\n",
            "triplet loss: tensor(0., grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1248, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.1157)\n",
            "total loss: 0.5969570433423363\n",
            "triplet loss: 1.7952220805826977\n",
            "cross entropy loss: 0.41743483290781636\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(92.1100)\n",
            "total loss: 0.3126845857354878\n",
            "triplet loss: tensor(0., grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3073, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.0509)\n",
            "total loss: 0.5725267994597819\n",
            "triplet loss: 1.5271738460783422\n",
            "cross entropy loss: 0.419809411461713\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(92.6267)\n",
            "total loss: 0.29548606020348794\n",
            "triplet loss: tensor(0.0063, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1442, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.6759)\n",
            "total loss: 0.5689845820062259\n",
            "triplet loss: 1.4403052298274972\n",
            "cross entropy loss: 0.4249540570704542\n",
            "margin: 0.005 lambda: 0.2\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(75.5567)\n",
            "total loss: 1.148158083973663\n",
            "triplet loss: tensor(0.3644, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.6587, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(84.5556)\n",
            "total loss: 0.7973741560998048\n",
            "triplet loss: 1.5052641658387946\n",
            "cross entropy loss: 0.4963213173154543\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(85.7683)\n",
            "total loss: 0.6353210005234046\n",
            "triplet loss: tensor(0.4382, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1104, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(85.7963)\n",
            "total loss: 0.6839641891640319\n",
            "triplet loss: 1.1245775021744904\n",
            "cross entropy loss: 0.45904868603105375\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(87.7250)\n",
            "total loss: 0.528187982738018\n",
            "triplet loss: tensor(0.2951, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.5741, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.2731)\n",
            "total loss: 0.6825739448592507\n",
            "triplet loss: 1.1780799207602732\n",
            "cross entropy loss: 0.44695795644846187\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(88.9750)\n",
            "total loss: 0.47052178110903514\n",
            "triplet loss: tensor(0.1394, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4693, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.4769)\n",
            "total loss: 0.6649187954925221\n",
            "triplet loss: 1.100542657650434\n",
            "cross entropy loss: 0.4448102622033929\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(89.7250)\n",
            "total loss: 0.4387292129430435\n",
            "triplet loss: tensor(0.1041, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2837, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.9583)\n",
            "total loss: 0.6937560840515342\n",
            "triplet loss: 1.258547682778017\n",
            "cross entropy loss: 0.44204654467440924\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(90.2500)\n",
            "total loss: 0.419030028000188\n",
            "triplet loss: tensor(0.1758, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1447, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.1019)\n",
            "total loss: 0.8053443317229931\n",
            "triplet loss: 1.6963539423762695\n",
            "cross entropy loss: 0.4660735396150301\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(90.6867)\n",
            "total loss: 0.3952078065638349\n",
            "triplet loss: tensor(0.0007, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1482, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.2130)\n",
            "total loss: 0.6251263937653875\n",
            "triplet loss: 1.002911028667138\n",
            "cross entropy loss: 0.42454418418763656\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(91.1117)\n",
            "total loss: 0.3707505813730297\n",
            "triplet loss: tensor(0.8477, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1693, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.1065)\n",
            "total loss: 0.6155914548762451\n",
            "triplet loss: 1.003548817896455\n",
            "cross entropy loss: 0.4148816867957454\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(91.5250)\n",
            "total loss: 0.35669735081192017\n",
            "triplet loss: tensor(0.1096, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3805, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.4954)\n",
            "total loss: 0.625274526296988\n",
            "triplet loss: 0.9834011282066445\n",
            "cross entropy loss: 0.42859429722795117\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(91.8167)\n",
            "total loss: 0.33758601752012524\n",
            "triplet loss: tensor(0.4798, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1040, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.8750)\n",
            "total loss: 0.6189984996643292\n",
            "triplet loss: 0.9760210539197781\n",
            "cross entropy loss: 0.4237942844188425\n",
            "margin: 0.005 lambda: 0.3\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(71.8983)\n",
            "total loss: 1.362987023617413\n",
            "triplet loss: tensor(0.6889, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.7829, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(82.8056)\n",
            "total loss: 0.9353769804246327\n",
            "triplet loss: 1.2324932554417107\n",
            "cross entropy loss: 0.5656289881503088\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(84.2883)\n",
            "total loss: 0.7306701758904244\n",
            "triplet loss: tensor(0.2863, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3466, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(84.7870)\n",
            "total loss: 0.7675547497512321\n",
            "triplet loss: 0.9194156707391231\n",
            "cross entropy loss: 0.4917300369965254\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(86.7967)\n",
            "total loss: 0.5950606191781029\n",
            "triplet loss: tensor(0.0003, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.7163, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.3657)\n",
            "total loss: 0.7073202604020136\n",
            "triplet loss: 0.8983506530346955\n",
            "cross entropy loss: 0.43781505520703523\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(87.9267)\n",
            "total loss: 0.5323877607359052\n",
            "triplet loss: tensor(0.4391, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1565, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.3194)\n",
            "total loss: 0.7163669690930632\n",
            "triplet loss: 0.8909286983234025\n",
            "cross entropy loss: 0.4490883474695612\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(88.7767)\n",
            "total loss: 0.4892247981830701\n",
            "triplet loss: tensor(0.6229, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1847, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.7454)\n",
            "total loss: 0.7025312446278228\n",
            "triplet loss: 0.8726434252009942\n",
            "cross entropy loss: 0.44073820431556926\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(89.4817)\n",
            "total loss: 0.4575109885477308\n",
            "triplet loss: tensor(0.0163, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2008, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.8657)\n",
            "total loss: 0.7311871438865831\n",
            "triplet loss: 0.9659819191090278\n",
            "cross entropy loss: 0.4413925577285727\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(89.8850)\n",
            "total loss: 0.4356438952929048\n",
            "triplet loss: tensor(0.0305, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2495, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.1806)\n",
            "total loss: 0.6975623121892912\n",
            "triplet loss: 0.8991520392361301\n",
            "cross entropy loss: 0.42781669020476426\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(90.5183)\n",
            "total loss: 0.4119126140626509\n",
            "triplet loss: tensor(0.2971, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3411, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.4444)\n",
            "total loss: 0.7038359350177663\n",
            "triplet loss: 0.8560293995879987\n",
            "cross entropy loss: 0.4470271051106368\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(90.8750)\n",
            "total loss: 0.3883155958770689\n",
            "triplet loss: tensor(0.3376, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1690, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.6343)\n",
            "total loss: 0.6538297479762833\n",
            "triplet loss: 0.7819090594190377\n",
            "cross entropy loss: 0.41925702259473546\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(91.3983)\n",
            "total loss: 0.3669666708039958\n",
            "triplet loss: tensor(0.0339, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4139, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.0787)\n",
            "total loss: 0.7215976704948047\n",
            "triplet loss: 0.9721271471456575\n",
            "cross entropy loss: 0.42995951496637785\n",
            "margin: 0.025 lambda: 0.1\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(77.3033)\n",
            "total loss: 0.999506935509029\n",
            "triplet loss: tensor(0.9420, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3584, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(85.4074)\n",
            "total loss: 0.6838156484816906\n",
            "triplet loss: 2.1532949461386752\n",
            "cross entropy loss: 0.46848614981188574\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(86.3033)\n",
            "total loss: 0.5827548499745346\n",
            "triplet loss: tensor(1.3094, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4426, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.2269)\n",
            "total loss: 0.6313000963284419\n",
            "triplet loss: 1.8129834747878757\n",
            "cross entropy loss: 0.4500017434182252\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(88.2550)\n",
            "total loss: 0.49149093602194205\n",
            "triplet loss: tensor(0.4255, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4680, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.2870)\n",
            "total loss: 0.5825676314929533\n",
            "triplet loss: 1.669628816331632\n",
            "cross entropy loss: 0.41560474609041353\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(89.4367)\n",
            "total loss: 0.43651715988543494\n",
            "triplet loss: tensor(0.1919, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2041, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.4028)\n",
            "total loss: 0.5649996392825651\n",
            "triplet loss: 1.7548811479523976\n",
            "cross entropy loss: 0.3895115209798488\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(90.1050)\n",
            "total loss: 0.3965582979052687\n",
            "triplet loss: tensor(0.2341, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3219, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.1944)\n",
            "total loss: 0.5433356558694642\n",
            "triplet loss: 1.4936126843535689\n",
            "cross entropy loss: 0.39397438547226804\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(90.7467)\n",
            "total loss: 0.37283816638150447\n",
            "triplet loss: tensor(0.4068, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2196, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.0741)\n",
            "total loss: 0.560540599627255\n",
            "triplet loss: 1.3175737837098054\n",
            "cross entropy loss: 0.42878321796302965\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(91.0733)\n",
            "total loss: 0.3499267064511522\n",
            "triplet loss: tensor(0.0838, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2749, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.9259)\n",
            "total loss: 0.5335568949227503\n",
            "triplet loss: 1.243067406250175\n",
            "cross entropy loss: 0.40925015202728954\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(91.7400)\n",
            "total loss: 0.32497162241210686\n",
            "triplet loss: tensor(0.6980, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0936, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.5972)\n",
            "total loss: 0.5796894321780233\n",
            "triplet loss: 1.542713991592269\n",
            "cross entropy loss: 0.4254180307351273\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(92.1433)\n",
            "total loss: 0.31283131744593445\n",
            "triplet loss: tensor(0., grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4777, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.5602)\n",
            "total loss: 0.526839508900981\n",
            "triplet loss: 1.3298286613374508\n",
            "cross entropy loss: 0.3938566416986948\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(92.4250)\n",
            "total loss: 0.2973976183507933\n",
            "triplet loss: tensor(0.1360, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1984, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.7824)\n",
            "total loss: 0.5532833712517157\n",
            "triplet loss: 1.3392361077154882\n",
            "cross entropy loss: 0.41935975823176685\n",
            "margin: 0.025 lambda: 0.2\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(75.2567)\n",
            "total loss: 1.15377099974069\n",
            "triplet loss: tensor(0.5512, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.6120, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(83.3194)\n",
            "total loss: 0.821850257776898\n",
            "triplet loss: 1.4106841100567191\n",
            "cross entropy loss: 0.539713431921231\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(85.5533)\n",
            "total loss: 0.6351613898489521\n",
            "triplet loss: tensor(0.6305, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2614, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(85.2546)\n",
            "total loss: 0.6917880378531281\n",
            "triplet loss: 1.0716470539261251\n",
            "cross entropy loss: 0.4774586234748716\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(87.8700)\n",
            "total loss: 0.5202911409742034\n",
            "triplet loss: tensor(0.2932, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2307, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.5556)\n",
            "total loss: 0.6906705211252856\n",
            "triplet loss: 1.2523637922513766\n",
            "cross entropy loss: 0.4401977561281983\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(88.9867)\n",
            "total loss: 0.4641016236722851\n",
            "triplet loss: tensor(0.6120, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3667, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.6991)\n",
            "total loss: 0.6414390494220356\n",
            "triplet loss: 1.056110972218965\n",
            "cross entropy loss: 0.4302168507459601\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(89.7817)\n",
            "total loss: 0.4231440630485254\n",
            "triplet loss: tensor(0.1491, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1483, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.9120)\n",
            "total loss: 0.6470874743260575\n",
            "triplet loss: 1.022605979440392\n",
            "cross entropy loss: 0.44256627658855985\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(90.4767)\n",
            "total loss: 0.40242776926011165\n",
            "triplet loss: tensor(0.2315, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3315, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.0648)\n",
            "total loss: 0.6295335620818053\n",
            "triplet loss: 0.9787255302209341\n",
            "cross entropy loss: 0.43378845101894714\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(91.0317)\n",
            "total loss: 0.37533988572482363\n",
            "triplet loss: tensor(0.0931, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1048, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.9444)\n",
            "total loss: 0.6345668095191555\n",
            "triplet loss: 1.098234169967693\n",
            "cross entropy loss: 0.414919972133178\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(91.3400)\n",
            "total loss: 0.3592322370740396\n",
            "triplet loss: tensor(0.0148, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.6577, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.7917)\n",
            "total loss: 0.6562590586715902\n",
            "triplet loss: 1.1037607806012828\n",
            "cross entropy loss: 0.4355068992624974\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(91.7000)\n",
            "total loss: 0.3436473974866717\n",
            "triplet loss: tensor(0.7248, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0830, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.6157)\n",
            "total loss: 0.6959040529100147\n",
            "triplet loss: 1.2927822392191408\n",
            "cross entropy loss: 0.43734760066637623\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(92.2550)\n",
            "total loss: 0.32584895455697455\n",
            "triplet loss: tensor(0.0486, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4343, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.1852)\n",
            "total loss: 0.6756095361603788\n",
            "triplet loss: 1.2233756744652744\n",
            "cross entropy loss: 0.43093439788948856\n",
            "margin: 0.025 lambda: 0.3\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(72.9417)\n",
            "total loss: 1.3342300770379334\n",
            "triplet loss: tensor(0.3957, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.5702, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(80.5972)\n",
            "total loss: 0.9660320112691123\n",
            "triplet loss: 1.1729941923590101\n",
            "cross entropy loss: 0.614133740141547\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(84.9950)\n",
            "total loss: 0.710235770958573\n",
            "triplet loss: tensor(0.1534, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3457, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(85.0556)\n",
            "total loss: 0.7935992876100822\n",
            "triplet loss: 1.0535670678026579\n",
            "cross entropy loss: 0.47752915441813554\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(87.3000)\n",
            "total loss: 0.574343894431586\n",
            "triplet loss: tensor(0.5038, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.6201, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.0463)\n",
            "total loss: 0.7580174912186064\n",
            "triplet loss: 0.9751658732369102\n",
            "cross entropy loss: 0.46546771787327423\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(88.5333)\n",
            "total loss: 0.5134392543070352\n",
            "triplet loss: tensor(0.1190, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2415, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.6713)\n",
            "total loss: 0.6933352155445596\n",
            "triplet loss: 0.8766360155268181\n",
            "cross entropy loss: 0.4303444012470499\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(89.4433)\n",
            "total loss: 0.48278171499210126\n",
            "triplet loss: tensor(0.0669, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3953, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.8704)\n",
            "total loss: 0.6884402812585323\n",
            "triplet loss: 0.8486005127562221\n",
            "cross entropy loss: 0.43386011766875987\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(90.0550)\n",
            "total loss: 0.44646548174782347\n",
            "triplet loss: tensor(0.0417, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2461, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.1667)\n",
            "total loss: 0.7066151866986907\n",
            "triplet loss: 0.9050096464756678\n",
            "cross entropy loss: 0.43511228025312254\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(90.6200)\n",
            "total loss: 0.4156490562360551\n",
            "triplet loss: tensor(0.4594, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3497, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.1296)\n",
            "total loss: 0.6574940531564182\n",
            "triplet loss: 0.7617144629568655\n",
            "cross entropy loss: 0.4289797019191395\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(90.9233)\n",
            "total loss: 0.4015857019006952\n",
            "triplet loss: tensor(0.1470, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2964, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.1713)\n",
            "total loss: 0.7435878016186889\n",
            "triplet loss: 1.029503956437111\n",
            "cross entropy loss: 0.43473660269168\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(91.4867)\n",
            "total loss: 0.3766342641066895\n",
            "triplet loss: tensor(0., grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3106, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.5509)\n",
            "total loss: 0.8725477678800476\n",
            "triplet loss: 1.3850701971519628\n",
            "cross entropy loss: 0.45702669412426694\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(91.6700)\n",
            "total loss: 0.36979006075147375\n",
            "triplet loss: tensor(0., grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3421, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.5185)\n",
            "total loss: 0.7149399982754295\n",
            "triplet loss: 0.9414445456481103\n",
            "cross entropy loss: 0.4325066266151575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Without Triplet Loss**"
      ],
      "metadata": {
        "id": "T7npC7Hbd5zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fit(0, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRbSP1g-52n9",
        "outputId": "35d09253-0aee-4b0a-f23f-f5cd337eaf42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(79.7267)\n",
            "total loss: 0.6454061842454013\n",
            "triplet loss: tensor(25.0355, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.5330, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.6898)\n",
            "total loss: 0.42346566681854825\n",
            "triplet loss: 28.07178208954941\n",
            "cross entropy loss: 0.42346566681854825\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(88.5000)\n",
            "total loss: 0.37234022509632336\n",
            "triplet loss: tensor(24.1905, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3695, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.3472)\n",
            "total loss: 0.38653587315165433\n",
            "triplet loss: 34.493383514810596\n",
            "cross entropy loss: 0.38653587315165433\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(90.5083)\n",
            "total loss: 0.31015816150999653\n",
            "triplet loss: tensor(34.6670, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3504, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.)\n",
            "total loss: 0.38369097181502176\n",
            "triplet loss: 38.49339676467625\n",
            "cross entropy loss: 0.38369097181502176\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(91.3983)\n",
            "total loss: 0.2770818998334187\n",
            "triplet loss: tensor(32.7555, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3266, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.4444)\n",
            "total loss: 0.38102124937804493\n",
            "triplet loss: 40.01134633735792\n",
            "cross entropy loss: 0.38102124937804493\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(92.1067)\n",
            "total loss: 0.25263645746179225\n",
            "triplet loss: tensor(33.6443, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4649, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.3472)\n",
            "total loss: 0.3759148945954777\n",
            "triplet loss: 41.81124245908839\n",
            "cross entropy loss: 0.3759148945954777\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(92.5883)\n",
            "total loss: 0.237442398711896\n",
            "triplet loss: tensor(36.2793, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4005, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.6944)\n",
            "total loss: 0.37378769128765227\n",
            "triplet loss: 44.93294415671445\n",
            "cross entropy loss: 0.37378769128765227\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(93.1033)\n",
            "total loss: 0.21697454255765308\n",
            "triplet loss: tensor(38.4656, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1924, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.1945)\n",
            "total loss: 0.39823818191356913\n",
            "triplet loss: 47.25116263620952\n",
            "cross entropy loss: 0.39823818191356913\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(93.5450)\n",
            "total loss: 0.20419547792230205\n",
            "triplet loss: tensor(49.5927, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1272, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.9305)\n",
            "total loss: 0.40711644813197956\n",
            "triplet loss: 48.23221049788435\n",
            "cross entropy loss: 0.40711644813197956\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(93.6900)\n",
            "total loss: 0.19841426600819267\n",
            "triplet loss: tensor(40.0787, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1296, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.7037)\n",
            "total loss: 0.375979110091572\n",
            "triplet loss: 47.975073255730805\n",
            "cross entropy loss: 0.375979110091572\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(94.0550)\n",
            "total loss: 0.18354304853096\n",
            "triplet loss: tensor(38.1768, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3053, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.8704)\n",
            "total loss: 0.40092513101869787\n",
            "triplet loss: 52.782852624295025\n",
            "cross entropy loss: 0.40092513101869787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.fc_img = nn.Linear(32 * 8 * 8, 256)\n",
        "        self.fc_combined = nn.Linear(512, 256)# Concatenated feature and image size\n",
        "        self.fc_domain = nn.Linear(256, 5)\n",
        "        self.fc_combined2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x_img, x_features):\n",
        "        x_img = self.conv1(x_img)\n",
        "        x_img = self.conv2(x_img)\n",
        "        x_img = x_img.view(x_img.size(0), -1)  # Flatten the image\n",
        "        x_img = F.relu(self.fc_img(x_img))\n",
        "\n",
        "        # Concatenate features and image representations\n",
        "        x_combined = torch.cat((x_img, x_features), dim=1)\n",
        "\n",
        "        # Fully connected layers for prediction\n",
        "        x_combined = F.relu(self.fc_combined(x_combined))\n",
        "        x_combined2 = F.softmax(self.fc_combined2(x_combined))\n",
        "        x_combined = F.softmax(self.fc_domain(x_combined))\n",
        "        return x_combined2, x_combined\n",
        "\n",
        "def fit(margin, lambd):\n",
        "  model = CombinedModel()\n",
        "  crossLoss = nn.CrossEntropyLoss()\n",
        "  triplet = triplet_loss(margin=margin)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  epochs = 10\n",
        "  for epoch in range(epochs):\n",
        "    print('epoch: ' + str(epoch))\n",
        "    model.train()\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    triplet_total = 0.0\n",
        "    entropy_total = 0.0\n",
        "\n",
        "    num_samples = len(train.dataset)\n",
        "    num_batches = len(train)\n",
        "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(train):\n",
        "\n",
        "      outs2, outs = model(images, features)\n",
        "\n",
        "      loss1 = crossLoss(outs2, digit_labels)\n",
        "      loss2 = triplet(outs, domain_labels)\n",
        "      total_loss = loss1 + lambd * loss2\n",
        "\n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      _, preds = torch.max(outs2, dim=1) # Explain, [N]\n",
        "      running_corrects += torch.sum(preds == digit_labels)\n",
        "      running_loss += total_loss.item()\n",
        "      triplet_total += loss2.item()\n",
        "      entropy_total += loss1.item()\n",
        "\n",
        "    epoch_acc = (running_corrects / num_samples) * 100\n",
        "    epoch_loss = (running_loss / num_batches)\n",
        "    triplet_total  = (triplet_total / num_batches)\n",
        "    entropy_total = (entropy_total / num_batches)\n",
        "    print('train')\n",
        "    print('accuaray: ' + str(epoch_acc))\n",
        "    print('total loss: ' + str(epoch_loss))\n",
        "    print('triplet loss: ' + str(loss2))\n",
        "    print('cross entropy loss: ' + str(loss1))\n",
        "    print('---------------')\n",
        "\n",
        "\n",
        "    num_samples_test = len(test.dataset)\n",
        "    num_batches_test = len(test)\n",
        "    running_corrects_test = 0\n",
        "    running_loss_test = 0.0\n",
        "    triplet_total = 0.0\n",
        "    entropy_total = 0.0\n",
        "\n",
        "\n",
        "    model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "    with torch.no_grad(): # explain\n",
        "        # more on torch.no_grad(): https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#disabling-gradient-tracking\n",
        "\n",
        "        for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(test): # Get a batch of Data\n",
        "\n",
        "            outputs2, outputs = model(images, features) # Forward Pass\n",
        "            loss = crossLoss(outputs2, digit_labels) # Compute Loss\n",
        "            loss2 = triplet(outputs, domain_labels)\n",
        "            total_loss = loss + lambd * loss2\n",
        "\n",
        "            _, preds = torch.max(outputs2, 1) #\n",
        "            running_corrects_test += torch.sum(preds == digit_labels)\n",
        "            running_loss_test += total_loss.item()\n",
        "            triplet_total += loss2.item()\n",
        "            entropy_total += loss.item()\n",
        "\n",
        "    test_acc = (running_corrects_test / num_samples_test) * 100\n",
        "    test_loss = (running_loss_test / num_batches_test)\n",
        "    triplet_total = (triplet_total / num_batches_test)\n",
        "    entropy_total = (entropy_total / num_batches_test)\n",
        "    print('test')\n",
        "    print('accuray: ' + str(test_acc))\n",
        "    print('total loss: ' + str(test_loss))\n",
        "    print('triplet loss: ' + str(triplet_total))\n",
        "    print('cross entropy loss: ' + str(entropy_total))\n",
        "\n",
        "fit(0.001, 0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LffvIYBL60pm",
        "outputId": "c85937d1-921f-4f9e-e640-fdfddc85e27f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-123c536bfdfd>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x_combined2 = F.softmax(self.fc_combined2(x_combined))\n",
            "<ipython-input-9-123c536bfdfd>:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x_combined = F.softmax(self.fc_domain(x_combined))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "accuaray: tensor(65.7900)\n",
            "total loss: 1.81421034155624\n",
            "triplet loss: tensor(0.0601, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.6865, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(78.1204)\n",
            "total loss: 1.6906726377955554\n",
            "triplet loss: 0.07405577548200915\n",
            "cross entropy loss: 1.683267061879649\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(81.2767)\n",
            "total loss: 1.6555807585401068\n",
            "triplet loss: tensor(0.0375, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.5166, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(86.3935)\n",
            "total loss: 1.606356649356481\n",
            "triplet loss: 0.06745992133603293\n",
            "cross entropy loss: 1.5996106563235175\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(88.9183)\n",
            "total loss: 1.5795863087751718\n",
            "triplet loss: tensor(0.0208, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.6388, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.5741)\n",
            "total loss: 1.5845978475181308\n",
            "triplet loss: 0.06653486660643089\n",
            "cross entropy loss: 1.57794436265731\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(91.0983)\n",
            "total loss: 1.5571944141692953\n",
            "triplet loss: tensor(0.0549, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.5514, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.8102)\n",
            "total loss: 1.5707683298714767\n",
            "triplet loss: 0.06427409314170215\n",
            "cross entropy loss: 1.5643409226067673\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(92.3133)\n",
            "total loss: 1.5432654761556368\n",
            "triplet loss: tensor(0.0120, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.5515, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.0694)\n",
            "total loss: 1.5619031300206156\n",
            "triplet loss: 0.016951947715464075\n",
            "cross entropy loss: 1.5602079365380417\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(93.1400)\n",
            "total loss: 1.5320363776770227\n",
            "triplet loss: tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.5840, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.9769)\n",
            "total loss: 1.5529775210386196\n",
            "triplet loss: 0.008762750485994628\n",
            "cross entropy loss: 1.5521012495255329\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(93.7083)\n",
            "total loss: 1.525640893719598\n",
            "triplet loss: tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.4926, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.7315)\n",
            "total loss: 1.554135684783642\n",
            "triplet loss: 0.005869716336983724\n",
            "cross entropy loss: 1.5535487144656435\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(93.9250)\n",
            "total loss: 1.522817572042632\n",
            "triplet loss: tensor(0.0030, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.4924, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.3102)\n",
            "total loss: 1.5484163912795705\n",
            "triplet loss: 0.003585996813713931\n",
            "cross entropy loss: 1.5480577935128523\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(94.1450)\n",
            "total loss: 1.5202912078229094\n",
            "triplet loss: tensor(0.0011, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.4929, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.2963)\n",
            "total loss: 1.5476619245032588\n",
            "triplet loss: 0.0022975928501429905\n",
            "cross entropy loss: 1.5474321672902305\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(94.6533)\n",
            "total loss: 1.5151250363667128\n",
            "triplet loss: tensor(0.0022, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.5237, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.0046)\n",
            "total loss: 1.5509506174798549\n",
            "triplet loss: 0.0022382922508456253\n",
            "cross entropy loss: 1.5507267872257346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.fc_img = nn.Linear(32 * 8 * 8, 256)\n",
        "        self.fc_combined = nn.Linear(512, 256)# Concatenated feature and image size\n",
        "        # self.fc_domain = nn.Linear(256, 5)\n",
        "        self.fc_combined2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x_img, x_features):\n",
        "        x_img = self.conv1(x_img)\n",
        "        x_img = self.conv2(x_img)\n",
        "        x_img = x_img.view(x_img.size(0), -1)  # Flatten the image\n",
        "        x_img = F.relu(self.fc_img(x_img))\n",
        "\n",
        "        # Concatenate features and image representations\n",
        "        x_combined = torch.cat((x_img, x_features), dim=1)\n",
        "\n",
        "        # Fully connected layers for prediction\n",
        "        x_combined = F.relu(self.fc_combined(x_combined))\n",
        "        x_combined2 = F.softmax(self.fc_combined2(x_combined), dim=1)\n",
        "        # x_combined = F.softmax(self.fc_domain(x_combined))\n",
        "        return x_combined2, x_combined\n",
        "\n",
        "def fit(margin, lambd):\n",
        "  model = CombinedModel()\n",
        "  crossLoss = nn.CrossEntropyLoss()\n",
        "  triplet = triplet_loss(margin=margin)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  epochs = 10\n",
        "  for epoch in range(epochs):\n",
        "    print('epoch: ' + str(epoch))\n",
        "    model.train()\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    triplet_total = 0.0\n",
        "    entropy_total = 0.0\n",
        "\n",
        "    num_samples = len(train.dataset)\n",
        "    num_batches = len(train)\n",
        "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(train):\n",
        "\n",
        "      outs2, outs = model(images, features)\n",
        "\n",
        "      loss1 = crossLoss(outs2, digit_labels)\n",
        "      loss2 = triplet(outs, domain_labels)\n",
        "      total_loss = loss1 + lambd * loss2\n",
        "\n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      _, preds = torch.max(outs2, dim=1) # Explain, [N]\n",
        "      running_corrects += torch.sum(preds == digit_labels)\n",
        "      running_loss += total_loss.item()\n",
        "      triplet_total += loss2.item()\n",
        "      entropy_total += loss1.item()\n",
        "\n",
        "    epoch_acc = (running_corrects / num_samples) * 100\n",
        "    epoch_loss = (running_loss / num_batches)\n",
        "    triplet_total  = (triplet_total / num_batches)\n",
        "    entropy_total = (entropy_total / num_batches)\n",
        "    print('train')\n",
        "    print('accuaray: ' + str(epoch_acc))\n",
        "    print('total loss: ' + str(epoch_loss))\n",
        "    print('triplet loss: ' + str(loss2))\n",
        "    print('cross entropy loss: ' + str(loss1))\n",
        "    print('---------------')\n",
        "\n",
        "\n",
        "    num_samples_test = len(test.dataset)\n",
        "    num_batches_test = len(test)\n",
        "    running_corrects_test = 0\n",
        "    running_loss_test = 0.0\n",
        "    triplet_total = 0.0\n",
        "    entropy_total = 0.0\n",
        "\n",
        "\n",
        "    model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "    with torch.no_grad(): # explain\n",
        "        # more on torch.no_grad(): https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#disabling-gradient-tracking\n",
        "\n",
        "        for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(test): # Get a batch of Data\n",
        "\n",
        "            outputs2, outputs = model(images, features) # Forward Pass\n",
        "            loss = crossLoss(outputs2, digit_labels) # Compute Loss\n",
        "            loss2 = triplet(outputs, domain_labels)\n",
        "            total_loss = loss + lambd * loss2\n",
        "\n",
        "            _, preds = torch.max(outputs2, 1) #\n",
        "            running_corrects_test += torch.sum(preds == digit_labels)\n",
        "            running_loss_test += total_loss.item()\n",
        "            triplet_total += loss2.item()\n",
        "            entropy_total += loss.item()\n",
        "\n",
        "    test_acc = (running_corrects_test / num_samples_test) * 100\n",
        "    test_loss = (running_loss_test / num_batches_test)\n",
        "    triplet_total = (triplet_total / num_batches_test)\n",
        "    entropy_total = (entropy_total / num_batches_test)\n",
        "    print('test')\n",
        "    print('accuray: ' + str(test_acc))\n",
        "    print('total loss: ' + str(test_loss))\n",
        "    print('triplet loss: ' + str(triplet_total))\n",
        "    print('cross entropy loss: ' + str(entropy_total))\n",
        "\n",
        "fit(0, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVtDNyRKBE_u",
        "outputId": "6d58bc22-3fc0-48ad-d0f6-c03ae200e520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(65.8900)\n",
            "total loss: 1.8040747909403558\n",
            "triplet loss: tensor(41.4092, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.8308, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(77.5000)\n",
            "total loss: 1.6876162234142686\n",
            "triplet loss: 58.20996106164695\n",
            "cross entropy loss: 1.6876162234142686\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(79.9800)\n",
            "total loss: 1.6621104519504475\n",
            "triplet loss: tensor(49.5086, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.6377, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(80.1435)\n",
            "total loss: 1.6598986634135953\n",
            "triplet loss: 71.04108897327671\n",
            "cross entropy loss: 1.6598986634135953\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(82.2317)\n",
            "total loss: 1.6390156632801618\n",
            "triplet loss: tensor(42.5883, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.6114, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(85.4306)\n",
            "total loss: 1.6088988311897368\n",
            "triplet loss: 77.13038900476941\n",
            "cross entropy loss: 1.6088988311897368\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(89.1567)\n",
            "total loss: 1.570394671420807\n",
            "triplet loss: tensor(61.5051, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.5772, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.6945)\n",
            "total loss: 1.5752596192105988\n",
            "triplet loss: 77.4151713918652\n",
            "cross entropy loss: 1.5752596192105988\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(90.8367)\n",
            "total loss: 1.5530280352655503\n",
            "triplet loss: tensor(63.8646, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.4898, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.8889)\n",
            "total loss: 1.5625974084498615\n",
            "triplet loss: 89.85898953499878\n",
            "cross entropy loss: 1.5625974084498615\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(91.7967)\n",
            "total loss: 1.5435957052051894\n",
            "triplet loss: tensor(79.9997, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.5385, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.4444)\n",
            "total loss: 1.55728844425382\n",
            "triplet loss: 91.12540985423432\n",
            "cross entropy loss: 1.55728844425382\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(92.2867)\n",
            "total loss: 1.5383689639919094\n",
            "triplet loss: tensor(75.2174, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.5055, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.0324)\n",
            "total loss: 1.5606744638561496\n",
            "triplet loss: 107.34603983410717\n",
            "cross entropy loss: 1.5606744638561496\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(93.0033)\n",
            "total loss: 1.5311166910982843\n",
            "triplet loss: tensor(100.4799, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.4860, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.1806)\n",
            "total loss: 1.5589279867488253\n",
            "triplet loss: 105.77591978445561\n",
            "cross entropy loss: 1.5589279867488253\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(93.3883)\n",
            "total loss: 1.527228325541848\n",
            "triplet loss: tensor(63.5683, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.4942, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.7778)\n",
            "total loss: 1.553065111651223\n",
            "triplet loss: 112.17061619504669\n",
            "cross entropy loss: 1.553065111651223\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(93.6433)\n",
            "total loss: 1.524721515331187\n",
            "triplet loss: tensor(82.3417, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(1.5263, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.8194)\n",
            "total loss: 1.5524921706442296\n",
            "triplet loss: 105.66308891702685\n",
            "cross entropy loss: 1.5524921706442296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Best Config For Model**"
      ],
      "metadata": {
        "id": "kjDl4MrueMY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.fc_img = nn.Linear(32 * 8 * 8, 256)\n",
        "        self.fc_combined = nn.Linear(512, 256)# Concatenated feature and image size\n",
        "        self.fc_domain = nn.Linear(256, 5)\n",
        "        self.fc_combined2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x_img, x_features):\n",
        "        x_img = self.conv1(x_img)\n",
        "        x_img = self.conv2(x_img)\n",
        "        x_img = x_img.view(x_img.size(0), -1)  # Flatten the image\n",
        "        x_img = F.relu(self.fc_img(x_img))\n",
        "\n",
        "        # Concatenate features and image representations\n",
        "        x_combined = torch.cat((x_img, x_features), dim=1)\n",
        "\n",
        "        # Fully connected layers for prediction\n",
        "        x_combined = F.relu(self.fc_combined(x_combined))\n",
        "        x_combined2 = self.fc_combined2(x_combined)\n",
        "        x_combined = self.fc_domain(x_combined)\n",
        "        return x_combined2, x_combined\n",
        "\n",
        "def fit(margin, lambd):\n",
        "  model = CombinedModel()\n",
        "  crossLoss = nn.CrossEntropyLoss()\n",
        "  triplet = triplet_loss(margin=margin)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  epochs = 10\n",
        "  for epoch in range(epochs):\n",
        "    print('epoch: ' + str(epoch))\n",
        "    model.train()\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    triplet_total = 0.0\n",
        "    entropy_total = 0.0\n",
        "\n",
        "    num_samples = len(train.dataset)\n",
        "    num_batches = len(train)\n",
        "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(train):\n",
        "\n",
        "      outs2, outs = model(images, features)\n",
        "\n",
        "      loss1 = crossLoss(outs2, digit_labels)\n",
        "      loss2 = triplet(outs, domain_labels)\n",
        "      total_loss = loss1 + lambd * loss2\n",
        "\n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      _, preds = torch.max(outs2, dim=1) # Explain, [N]\n",
        "      running_corrects += torch.sum(preds == digit_labels)\n",
        "      running_loss += total_loss.item()\n",
        "      triplet_total += loss2.item()\n",
        "      entropy_total += loss1.item()\n",
        "\n",
        "    epoch_acc = (running_corrects / num_samples) * 100\n",
        "    epoch_loss = (running_loss / num_batches)\n",
        "    triplet_total  = (triplet_total / num_batches)\n",
        "    entropy_total = (entropy_total / num_batches)\n",
        "    print('train')\n",
        "    print('accuaray: ' + str(epoch_acc))\n",
        "    print('total loss: ' + str(epoch_loss))\n",
        "    print('triplet loss: ' + str(loss2))\n",
        "    print('cross entropy loss: ' + str(loss1))\n",
        "    print('---------------')\n",
        "\n",
        "\n",
        "    num_samples_test = len(test.dataset)\n",
        "    num_batches_test = len(test)\n",
        "    running_corrects_test = 0\n",
        "    running_loss_test = 0.0\n",
        "    triplet_total = 0.0\n",
        "    entropy_total = 0.0\n",
        "\n",
        "\n",
        "    model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "    with torch.no_grad(): # explain\n",
        "        # more on torch.no_grad(): https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#disabling-gradient-tracking\n",
        "\n",
        "        for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(test): # Get a batch of Data\n",
        "\n",
        "            outputs2, outputs = model(images, features) # Forward Pass\n",
        "            loss = crossLoss(outputs2, digit_labels) # Compute Loss\n",
        "            loss2 = triplet(outputs, domain_labels)\n",
        "            total_loss = loss + lambd * loss2\n",
        "\n",
        "            _, preds = torch.max(outputs2, 1) #\n",
        "            running_corrects_test += torch.sum(preds == digit_labels)\n",
        "            running_loss_test += total_loss.item()\n",
        "            triplet_total += loss2.item()\n",
        "            entropy_total += loss.item()\n",
        "\n",
        "    test_acc = (running_corrects_test / num_samples_test) * 100\n",
        "    test_loss = (running_loss_test / num_batches_test)\n",
        "    triplet_total = (triplet_total / num_batches_test)\n",
        "    entropy_total = (entropy_total / num_batches_test)\n",
        "    print('test')\n",
        "    print('accuray: ' + str(test_acc))\n",
        "    print('total loss: ' + str(test_loss))\n",
        "    print('triplet loss: ' + str(triplet_total))\n",
        "    print('cross entropy loss: ' + str(entropy_total))\n",
        "\n",
        "fit(0.002, 0.15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2TPStGHFLIV",
        "outputId": "09b0ebfd-b68a-4736-ec68-a50cd53e21be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(79.0017)\n",
            "total loss: 0.7036819048940754\n",
            "triplet loss: tensor(0.1648, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2074, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.8380)\n",
            "total loss: 0.39766276142829976\n",
            "triplet loss: 0.152366120324156\n",
            "cross entropy loss: 0.37480784273711887\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(91.1250)\n",
            "total loss: 0.3031704070122003\n",
            "triplet loss: tensor(0.0706, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4061, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.5509)\n",
            "total loss: 0.3359284945625878\n",
            "triplet loss: 0.12238581209669452\n",
            "cross entropy loss: 0.317570622216844\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(93.8550)\n",
            "total loss: 0.2080484930711832\n",
            "triplet loss: tensor(0.0399, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1354, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.3981)\n",
            "total loss: 0.30222724602948986\n",
            "triplet loss: 0.09933607013532396\n",
            "cross entropy loss: 0.2873268345778687\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(95.6650)\n",
            "total loss: 0.15293231335625465\n",
            "triplet loss: tensor(0.0512, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1778, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9583)\n",
            "total loss: 0.301191116148639\n",
            "triplet loss: 0.09536969569131468\n",
            "cross entropy loss: 0.28688566162961826\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(96.5883)\n",
            "total loss: 0.11994933649532195\n",
            "triplet loss: tensor(0.0575, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0624, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0370)\n",
            "total loss: 0.31458484828913\n",
            "triplet loss: 0.10783849092277549\n",
            "cross entropy loss: 0.29840907393872035\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.6483)\n",
            "total loss: 0.0891458196714441\n",
            "triplet loss: tensor(0.0697, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1517, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9583)\n",
            "total loss: 0.3290997339445635\n",
            "triplet loss: 0.1049694972134818\n",
            "cross entropy loss: 0.3133543086563342\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(98.1433)\n",
            "total loss: 0.07420649018480198\n",
            "triplet loss: tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0177, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0463)\n",
            "total loss: 0.3531531855552154\n",
            "triplet loss: 0.12233824174873223\n",
            "cross entropy loss: 0.33480244828663636\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.5200)\n",
            "total loss: 0.06217654612260873\n",
            "triplet loss: tensor(0.0846, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0142, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.7917)\n",
            "total loss: 0.36582075248984897\n",
            "triplet loss: 0.12509470240119294\n",
            "cross entropy loss: 0.3470565462044093\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.6150)\n",
            "total loss: 0.05886574251267479\n",
            "triplet loss: tensor(0.0306, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0580, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.8935)\n",
            "total loss: 0.3926247805136548\n",
            "triplet loss: 0.13026001816891\n",
            "cross entropy loss: 0.37308577737445836\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(99.1333)\n",
            "total loss: 0.04266852496920634\n",
            "triplet loss: tensor(0.0366, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0984, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9352)\n",
            "total loss: 0.4142451678022478\n",
            "triplet loss: 0.1278907139372279\n",
            "cross entropy loss: 0.3950615595149967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing A Few Values For Margin And Lambda**"
      ],
      "metadata": {
        "id": "zJx1J32NeVj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.fc_img = nn.Linear(32 * 8 * 8, 256)\n",
        "        self.fc_combined = nn.Linear(512, 256)# Concatenated feature and image size\n",
        "        self.fc_domain = nn.Linear(256, 5)\n",
        "        self.fc_combined2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x_img, x_features):\n",
        "        x_img = self.conv1(x_img)\n",
        "        x_img = self.conv2(x_img)\n",
        "        x_img = x_img.view(x_img.size(0), -1)  # Flatten the image\n",
        "        x_img = F.relu(self.fc_img(x_img))\n",
        "\n",
        "        # Concatenate features and image representations\n",
        "        x_combined = torch.cat((x_img, x_features), dim=1)\n",
        "\n",
        "        # Fully connected layers for prediction\n",
        "        x_combined = F.relu(self.fc_combined(x_combined))\n",
        "        x_combined2 = self.fc_combined2(x_combined)\n",
        "        x_combined = self.fc_domain(x_combined)\n",
        "        return x_combined2, x_combined\n",
        "\n",
        "def fit(margin, lambd):\n",
        "  model = CombinedModel()\n",
        "  crossLoss = nn.CrossEntropyLoss()\n",
        "  triplet = triplet_loss(margin=margin)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  epochs = 10\n",
        "  for epoch in range(epochs):\n",
        "    print('epoch: ' + str(epoch))\n",
        "    model.train()\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    triplet_total = 0.0\n",
        "    entropy_total = 0.0\n",
        "\n",
        "    num_samples = len(train.dataset)\n",
        "    num_batches = len(train)\n",
        "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(train):\n",
        "\n",
        "      outs2, outs = model(images, features)\n",
        "\n",
        "      loss1 = crossLoss(outs2, digit_labels)\n",
        "      loss2 = triplet(outs, domain_labels)\n",
        "      total_loss = loss1 + lambd * loss2\n",
        "\n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      _, preds = torch.max(outs2, dim=1) # Explain, [N]\n",
        "      running_corrects += torch.sum(preds == digit_labels)\n",
        "      running_loss += total_loss.item()\n",
        "      triplet_total += loss2.item()\n",
        "      entropy_total += loss1.item()\n",
        "\n",
        "    epoch_acc = (running_corrects / num_samples) * 100\n",
        "    epoch_loss = (running_loss / num_batches)\n",
        "    triplet_total  = (triplet_total / num_batches)\n",
        "    entropy_total = (entropy_total / num_batches)\n",
        "    print('train')\n",
        "    print('accuaray: ' + str(epoch_acc))\n",
        "    print('total loss: ' + str(epoch_loss))\n",
        "    print('triplet loss: ' + str(loss2))\n",
        "    print('cross entropy loss: ' + str(loss1))\n",
        "    print('---------------')\n",
        "\n",
        "\n",
        "    num_samples_test = len(test.dataset)\n",
        "    num_batches_test = len(test)\n",
        "    running_corrects_test = 0\n",
        "    running_loss_test = 0.0\n",
        "    triplet_total = 0.0\n",
        "    entropy_total = 0.0\n",
        "\n",
        "\n",
        "    model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "    with torch.no_grad(): # explain\n",
        "        # more on torch.no_grad(): https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#disabling-gradient-tracking\n",
        "\n",
        "        for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(test): # Get a batch of Data\n",
        "\n",
        "            outputs2, outputs = model(images, features) # Forward Pass\n",
        "            loss = crossLoss(outputs2, digit_labels) # Compute Loss\n",
        "            loss2 = triplet(outputs, domain_labels)\n",
        "            total_loss = loss + lambd * loss2\n",
        "\n",
        "            _, preds = torch.max(outputs2, 1) #\n",
        "            running_corrects_test += torch.sum(preds == digit_labels)\n",
        "            running_loss_test += total_loss.item()\n",
        "            triplet_total += loss2.item()\n",
        "            entropy_total += loss.item()\n",
        "\n",
        "    test_acc = (running_corrects_test / num_samples_test) * 100\n",
        "    test_loss = (running_loss_test / num_batches_test)\n",
        "    triplet_total = (triplet_total / num_batches_test)\n",
        "    entropy_total = (entropy_total / num_batches_test)\n",
        "    print('test')\n",
        "    print('accuray: ' + str(test_acc))\n",
        "    print('total loss: ' + str(test_loss))\n",
        "    print('triplet loss: ' + str(triplet_total))\n",
        "    print('cross entropy loss: ' + str(entropy_total))\n",
        "\n",
        "for m in [0.0005, 0.001, 0.002, 0.004, 0.008]:\n",
        "  for l in [0.1, 0.2, 0.4]:\n",
        "    print('margin: ' + str(m) + ' lambda: ' + str(l))\n",
        "    fit(margin=m, lambd=l)\n",
        "# best fit(0.008, 0.4)"
      ],
      "metadata": {
        "id": "Pe8CszNvci-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca99835c-ba9d-42ee-fa1d-50499820c723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "margin: 0.0005 lambda: 0.1\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(79.7717)\n",
            "total loss: 0.6555821264047486\n",
            "triplet loss: tensor(0.1214, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1846, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.9907)\n",
            "total loss: 0.3788883471806374\n",
            "triplet loss: 0.17401118543461935\n",
            "cross entropy loss: 0.36148722890830604\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(91.4417)\n",
            "total loss: 0.28537420523382706\n",
            "triplet loss: tensor(0.0482, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0333, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.5741)\n",
            "total loss: 0.32802140902309024\n",
            "triplet loss: 0.13903966109604526\n",
            "cross entropy loss: 0.3141174419127272\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(94.1833)\n",
            "total loss: 0.19949701175030107\n",
            "triplet loss: tensor(0.1341, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1973, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.6435)\n",
            "total loss: 0.3003754323932546\n",
            "triplet loss: 0.11998610371536404\n",
            "cross entropy loss: 0.2883768214364729\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(95.7300)\n",
            "total loss: 0.14847587753555921\n",
            "triplet loss: tensor(0.1008, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0463, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.5926)\n",
            "total loss: 0.3085944845214398\n",
            "triplet loss: 0.13264697932324113\n",
            "cross entropy loss: 0.2953297863741951\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(96.7200)\n",
            "total loss: 0.11463261364516356\n",
            "triplet loss: tensor(0.0659, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0860, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3750)\n",
            "total loss: 0.30507602306777204\n",
            "triplet loss: 0.12536073062767292\n",
            "cross entropy loss: 0.29253995088452595\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.5550)\n",
            "total loss: 0.08938797259294211\n",
            "triplet loss: tensor(0.0816, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1399, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1898)\n",
            "total loss: 0.33040937771414514\n",
            "triplet loss: 0.12278396994448625\n",
            "cross entropy loss: 0.3181309809869695\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(98.2067)\n",
            "total loss: 0.06860734118935047\n",
            "triplet loss: tensor(0.0810, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1315, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0278)\n",
            "total loss: 0.34895976393665434\n",
            "triplet loss: 0.14824823595407094\n",
            "cross entropy loss: 0.33413494013514394\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.5950)\n",
            "total loss: 0.058621226729495504\n",
            "triplet loss: tensor(0.0355, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0061, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0741)\n",
            "total loss: 0.3752923595702507\n",
            "triplet loss: 0.1372425550231214\n",
            "cross entropy loss: 0.3615681040514062\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.6100)\n",
            "total loss: 0.056089224271488186\n",
            "triplet loss: tensor(0.0979, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0390, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1574)\n",
            "total loss: 0.3901694225004086\n",
            "triplet loss: 0.15976485252909406\n",
            "cross entropy loss: 0.37419293684190547\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(98.9350)\n",
            "total loss: 0.04459845022346054\n",
            "triplet loss: tensor(0.0399, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0022, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3519)\n",
            "total loss: 0.3834919390329242\n",
            "triplet loss: 0.1322396491714836\n",
            "cross entropy loss: 0.37026797440013237\n",
            "margin: 0.0005 lambda: 0.2\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(78.3333)\n",
            "total loss: 0.722898668095247\n",
            "triplet loss: tensor(0.1221, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.5233, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.5463)\n",
            "total loss: 0.40869411539572936\n",
            "triplet loss: 0.1482457828063231\n",
            "cross entropy loss: 0.3790449594428553\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(91.0817)\n",
            "total loss: 0.3101364514196732\n",
            "triplet loss: tensor(0.0528, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3458, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.6343)\n",
            "total loss: 0.31832563652265705\n",
            "triplet loss: 0.07882034922977524\n",
            "cross entropy loss: 0.3025615671374036\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(93.8967)\n",
            "total loss: 0.2140486400161407\n",
            "triplet loss: tensor(0.0849, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3235, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.4167)\n",
            "total loss: 0.32156582562440245\n",
            "triplet loss: 0.10386520166765656\n",
            "cross entropy loss: 0.3007927851079131\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(95.6517)\n",
            "total loss: 0.15600636602639517\n",
            "triplet loss: tensor(0.0797, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0748, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.6019)\n",
            "total loss: 0.32354057977009104\n",
            "triplet loss: 0.11392116498135955\n",
            "cross entropy loss: 0.3007563467760234\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(96.7933)\n",
            "total loss: 0.11805770444924008\n",
            "triplet loss: tensor(0.0299, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0442, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3519)\n",
            "total loss: 0.3200820884805107\n",
            "triplet loss: 0.12792276414717443\n",
            "cross entropy loss: 0.2944975353380632\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.6117)\n",
            "total loss: 0.09313795953067636\n",
            "triplet loss: tensor(0.0642, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0194, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3148)\n",
            "total loss: 0.329164650302226\n",
            "triplet loss: 0.10284148028792714\n",
            "cross entropy loss: 0.3085963545620618\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(98.0383)\n",
            "total loss: 0.07933070386912841\n",
            "triplet loss: tensor(0.1266, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0532, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0926)\n",
            "total loss: 0.3522634421359505\n",
            "triplet loss: 0.13070092378900602\n",
            "cross entropy loss: 0.3261232576746295\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.5600)\n",
            "total loss: 0.06312564552934376\n",
            "triplet loss: tensor(0.0356, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0070, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.2870)\n",
            "total loss: 0.36159956717544056\n",
            "triplet loss: 0.10995085761722552\n",
            "cross entropy loss: 0.33960939525521894\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.7800)\n",
            "total loss: 0.053571742039912544\n",
            "triplet loss: tensor(0.0044, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0348, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0093)\n",
            "total loss: 0.3755988404705856\n",
            "triplet loss: 0.11410106417100281\n",
            "cross entropy loss: 0.35277862684283207\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(99.0250)\n",
            "total loss: 0.04575198504235794\n",
            "triplet loss: tensor(0.0193, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0262, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.4537)\n",
            "total loss: 0.38025224686065723\n",
            "triplet loss: 0.1289868116180396\n",
            "cross entropy loss: 0.35445488415680515\n",
            "margin: 0.0005 lambda: 0.4\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(78.0383)\n",
            "total loss: 0.7590050698438687\n",
            "triplet loss: tensor(0.0582, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2559, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.4907)\n",
            "total loss: 0.4172073156523281\n",
            "triplet loss: 0.09924364033755818\n",
            "cross entropy loss: 0.37750985905439893\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(91.0283)\n",
            "total loss: 0.3194950016370333\n",
            "triplet loss: tensor(0.0441, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4352, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.6852)\n",
            "total loss: 0.33988491062229204\n",
            "triplet loss: 0.09119261050673984\n",
            "cross entropy loss: 0.30340786659594116\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(93.7983)\n",
            "total loss: 0.2246166524419716\n",
            "triplet loss: tensor(5.1916e-05, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2977, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9352)\n",
            "total loss: 0.2996029980789275\n",
            "triplet loss: 0.08320193005913108\n",
            "cross entropy loss: 0.2663222259825327\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(95.4367)\n",
            "total loss: 0.16949502631092567\n",
            "triplet loss: tensor(0.0252, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1246, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0370)\n",
            "total loss: 0.301592560980976\n",
            "triplet loss: 0.07890659995024374\n",
            "cross entropy loss: 0.27002992053356395\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(96.6117)\n",
            "total loss: 0.13093181110934407\n",
            "triplet loss: tensor(0.0450, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0334, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3657)\n",
            "total loss: 0.3024357645248873\n",
            "triplet loss: 0.07911278989673014\n",
            "cross entropy loss: 0.270790648255387\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.5250)\n",
            "total loss: 0.10111890641301235\n",
            "triplet loss: tensor(0.0386, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0910, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.2731)\n",
            "total loss: 0.33476080944949\n",
            "triplet loss: 0.09008905112732356\n",
            "cross entropy loss: 0.2987251879382857\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(98.0517)\n",
            "total loss: 0.08409648397262258\n",
            "triplet loss: tensor(0.0202, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0138, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.2407)\n",
            "total loss: 0.3437379273150623\n",
            "triplet loss: 0.09075779465616808\n",
            "cross entropy loss: 0.3074348090470014\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.3333)\n",
            "total loss: 0.07171521120186426\n",
            "triplet loss: tensor(0.0346, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1301, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.6667)\n",
            "total loss: 0.4054503496295425\n",
            "triplet loss: 0.0978775071021522\n",
            "cross entropy loss: 0.36629934511395423\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.6850)\n",
            "total loss: 0.05856306567442601\n",
            "triplet loss: tensor(0.0079, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0009, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0139)\n",
            "total loss: 0.3836244078871061\n",
            "triplet loss: 0.11574356031697794\n",
            "cross entropy loss: 0.3373269833425975\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(98.9600)\n",
            "total loss: 0.04889113799324319\n",
            "triplet loss: tensor(0.0035, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1670, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0833)\n",
            "total loss: 0.4239416716388873\n",
            "triplet loss: 0.11737666919981764\n",
            "cross entropy loss: 0.37699100344590625\n",
            "margin: 0.001 lambda: 0.1\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(77.9833)\n",
            "total loss: 0.7158038009490285\n",
            "triplet loss: tensor(0.1091, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.5042, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.1481)\n",
            "total loss: 0.39174139393857244\n",
            "triplet loss: 0.16234168511699643\n",
            "cross entropy loss: 0.3755072253431089\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(90.9633)\n",
            "total loss: 0.30944705379606563\n",
            "triplet loss: tensor(0.1366, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1227, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.8519)\n",
            "total loss: 0.3215988065672697\n",
            "triplet loss: 0.1352653692768523\n",
            "cross entropy loss: 0.3080722693596366\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(93.6350)\n",
            "total loss: 0.21632174925922332\n",
            "triplet loss: tensor(0.0363, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2903, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9167)\n",
            "total loss: 0.2868243990763581\n",
            "triplet loss: 0.10275465004234272\n",
            "cross entropy loss: 0.2765489341746244\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(95.3433)\n",
            "total loss: 0.1584118274844754\n",
            "triplet loss: tensor(0.0650, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1861, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.4120)\n",
            "total loss: 0.3026035532752085\n",
            "triplet loss: 0.12807676094699894\n",
            "cross entropy loss: 0.28979587702069587\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(96.6117)\n",
            "total loss: 0.11952721198270125\n",
            "triplet loss: tensor(0.0846, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1231, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3611)\n",
            "total loss: 0.3119513648699903\n",
            "triplet loss: 0.11339867749863122\n",
            "cross entropy loss: 0.30061149729426795\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.2867)\n",
            "total loss: 0.09592622333664948\n",
            "triplet loss: tensor(0.1020, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0251, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.2593)\n",
            "total loss: 0.3164183388205146\n",
            "triplet loss: 0.1424226742678669\n",
            "cross entropy loss: 0.30217607116007067\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(97.9400)\n",
            "total loss: 0.07744222452173005\n",
            "triplet loss: tensor(0.0606, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0468, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3055)\n",
            "total loss: 0.33071146853620836\n",
            "triplet loss: 0.12973240962439386\n",
            "cross entropy loss: 0.31773822697860427\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.3033)\n",
            "total loss: 0.0654574279854102\n",
            "triplet loss: tensor(0.1022, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0163, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3750)\n",
            "total loss: 0.35081589281470815\n",
            "triplet loss: 0.1369646969168673\n",
            "cross entropy loss: 0.33711942347020085\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.6767)\n",
            "total loss: 0.0531570147166946\n",
            "triplet loss: tensor(0.1937, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0668, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.9074)\n",
            "total loss: 0.3449346625284683\n",
            "triplet loss: 0.1519333328665978\n",
            "cross entropy loss: 0.3297413295305736\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(98.8517)\n",
            "total loss: 0.04804746420415583\n",
            "triplet loss: tensor(0.0951, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0900, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9861)\n",
            "total loss: 0.38068366503706697\n",
            "triplet loss: 0.14646849677672047\n",
            "cross entropy loss: 0.36603681502874785\n",
            "margin: 0.001 lambda: 0.2\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(79.0983)\n",
            "total loss: 0.6997096532665844\n",
            "triplet loss: tensor(0.0652, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1477, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.7963)\n",
            "total loss: 0.3957223923072307\n",
            "triplet loss: 0.1307732344290914\n",
            "cross entropy loss: 0.3695677454478642\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(91.1617)\n",
            "total loss: 0.3036041549488362\n",
            "triplet loss: tensor(0.0686, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4116, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.8565)\n",
            "total loss: 0.32407347948328985\n",
            "triplet loss: 0.0870196438620429\n",
            "cross entropy loss: 0.3066695514383048\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(93.7367)\n",
            "total loss: 0.21256929414390502\n",
            "triplet loss: tensor(0.0306, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1153, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.4583)\n",
            "total loss: 0.2995500561048293\n",
            "triplet loss: 0.09390944691055272\n",
            "cross entropy loss: 0.2807681667425576\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(95.5267)\n",
            "total loss: 0.1557747944832039\n",
            "triplet loss: tensor(0.1316, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1460, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.7824)\n",
            "total loss: 0.3144957894051569\n",
            "triplet loss: 0.1008400517851996\n",
            "cross entropy loss: 0.294327779645486\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(96.8900)\n",
            "total loss: 0.11818307442570737\n",
            "triplet loss: tensor(0.0595, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0837, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1667)\n",
            "total loss: 0.3180827749286883\n",
            "triplet loss: 0.1162475411333805\n",
            "cross entropy loss: 0.29483326636365004\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.6283)\n",
            "total loss: 0.0931492050839608\n",
            "triplet loss: tensor(0.0525, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0302, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9444)\n",
            "total loss: 0.3419438779750872\n",
            "triplet loss: 0.11280986777997229\n",
            "cross entropy loss: 0.3193819051354947\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(98.1383)\n",
            "total loss: 0.07723996855382091\n",
            "triplet loss: tensor(0.1059, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2010, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.4398)\n",
            "total loss: 0.3852193625147879\n",
            "triplet loss: 0.11856283359339781\n",
            "cross entropy loss: 0.3615067956869948\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.6250)\n",
            "total loss: 0.062439121364522523\n",
            "triplet loss: tensor(0.0196, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1452, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.7546)\n",
            "total loss: 0.39309428297203675\n",
            "triplet loss: 0.11196919737835608\n",
            "cross entropy loss: 0.37070044333985924\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.7417)\n",
            "total loss: 0.05395810323993741\n",
            "triplet loss: tensor(0.1614, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2755, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9676)\n",
            "total loss: 0.3864121876282276\n",
            "triplet loss: 0.116527675595906\n",
            "cross entropy loss: 0.36310665221256616\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(99.0450)\n",
            "total loss: 0.044247146391037745\n",
            "triplet loss: tensor(0.0286, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0023, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1296)\n",
            "total loss: 0.40382305414763076\n",
            "triplet loss: 0.12548252820593894\n",
            "cross entropy loss: 0.3787265490112308\n",
            "margin: 0.001 lambda: 0.4\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(79.1383)\n",
            "total loss: 0.7167499913399153\n",
            "triplet loss: tensor(0.0337, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3468, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.6574)\n",
            "total loss: 0.3936895801735348\n",
            "triplet loss: 0.08651762909790468\n",
            "cross entropy loss: 0.3590825287371697\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(91.5700)\n",
            "total loss: 0.3000700689359769\n",
            "triplet loss: tensor(0.0545, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3945, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.9676)\n",
            "total loss: 0.3315495118146112\n",
            "triplet loss: 0.0787124949336581\n",
            "cross entropy loss: 0.30006451424233305\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(94.1383)\n",
            "total loss: 0.2124515651171205\n",
            "triplet loss: tensor(0.0830, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1095, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.5417)\n",
            "total loss: 0.30257883859721163\n",
            "triplet loss: 0.07320745009928942\n",
            "cross entropy loss: 0.2732958580262562\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(95.8617)\n",
            "total loss: 0.15569095125298765\n",
            "triplet loss: tensor(0.0091, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0907, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0926)\n",
            "total loss: 0.30009558933550085\n",
            "triplet loss: 0.07713781794004099\n",
            "cross entropy loss: 0.26924046152133563\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(96.9700)\n",
            "total loss: 0.11844503113638553\n",
            "triplet loss: tensor(0.0146, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1519, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1713)\n",
            "total loss: 0.3285218504472421\n",
            "triplet loss: 0.09536310822080225\n",
            "cross entropy loss: 0.2903766071357759\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.6250)\n",
            "total loss: 0.09626888803271914\n",
            "triplet loss: tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1120, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.4815)\n",
            "total loss: 0.335318047575344\n",
            "triplet loss: 0.09045616633702377\n",
            "cross entropy loss: 0.29913557977718713\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(98.0617)\n",
            "total loss: 0.07744073214482015\n",
            "triplet loss: tensor(0.0292, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0385, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3843)\n",
            "total loss: 0.3472592293025827\n",
            "triplet loss: 0.1079349521941577\n",
            "cross entropy loss: 0.30408524759885297\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.6200)\n",
            "total loss: 0.06004140600174872\n",
            "triplet loss: tensor(0.0228, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0485, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.4444)\n",
            "total loss: 0.36949348995274695\n",
            "triplet loss: 0.10148860341400878\n",
            "cross entropy loss: 0.32889804800630673\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.6150)\n",
            "total loss: 0.059032871840652756\n",
            "triplet loss: tensor(0.0090, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0178, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1574)\n",
            "total loss: 0.3903652442806571\n",
            "triplet loss: 0.10598601369785156\n",
            "cross entropy loss: 0.3479708378563481\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(98.9083)\n",
            "total loss: 0.04574735137695538\n",
            "triplet loss: tensor(0.0244, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0184, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.4630)\n",
            "total loss: 0.4051620724153589\n",
            "triplet loss: 0.11933110049993252\n",
            "cross entropy loss: 0.3574296309662289\n",
            "margin: 0.002 lambda: 0.1\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(79.2417)\n",
            "total loss: 0.6754216881258401\n",
            "triplet loss: tensor(0.1791, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2241, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.6204)\n",
            "total loss: 0.3581639403510376\n",
            "triplet loss: 0.1690740082523174\n",
            "cross entropy loss: 0.34125653890198504\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(91.3383)\n",
            "total loss: 0.29278997272284807\n",
            "triplet loss: tensor(0.0740, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1102, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.9769)\n",
            "total loss: 0.34003395529893726\n",
            "triplet loss: 0.12914489540299015\n",
            "cross entropy loss: 0.3271194654103567\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(93.9533)\n",
            "total loss: 0.20041073727677625\n",
            "triplet loss: tensor(0.1130, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.5749, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.7130)\n",
            "total loss: 0.28860200220752047\n",
            "triplet loss: 0.10714719744299996\n",
            "cross entropy loss: 0.2778872823750479\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(95.8000)\n",
            "total loss: 0.1463228371709204\n",
            "triplet loss: tensor(0.0241, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3267, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.6667)\n",
            "total loss: 0.30254822909920176\n",
            "triplet loss: 0.1241536024274558\n",
            "cross entropy loss: 0.29013286890274675\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(96.9000)\n",
            "total loss: 0.10881393265002953\n",
            "triplet loss: tensor(0.0292, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2797, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0185)\n",
            "total loss: 0.3102619476704555\n",
            "triplet loss: 0.14230604972359698\n",
            "cross entropy loss: 0.2960313422407007\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.5583)\n",
            "total loss: 0.08581738368467068\n",
            "triplet loss: tensor(0.0805, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1972, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.2361)\n",
            "total loss: 0.3333477296552362\n",
            "triplet loss: 0.12215014152537436\n",
            "cross entropy loss: 0.32113271548561534\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(98.1017)\n",
            "total loss: 0.0697138545923491\n",
            "triplet loss: tensor(0.0610, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1356, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.7454)\n",
            "total loss: 0.3559260255265871\n",
            "triplet loss: 0.1358577943897459\n",
            "cross entropy loss: 0.3423402460688758\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.6433)\n",
            "total loss: 0.05412447859527174\n",
            "triplet loss: tensor(0.1510, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0075, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3426)\n",
            "total loss: 0.36349961587046026\n",
            "triplet loss: 0.1358453865829833\n",
            "cross entropy loss: 0.34991507690135365\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.6550)\n",
            "total loss: 0.052325900927035096\n",
            "triplet loss: tensor(0.0153, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1170, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3241)\n",
            "total loss: 0.3761504737483944\n",
            "triplet loss: 0.1482900875515839\n",
            "cross entropy loss: 0.36132146423880956\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(98.9183)\n",
            "total loss: 0.044141514877379655\n",
            "triplet loss: tensor(0.0609, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0101, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0185)\n",
            "total loss: 0.40990527114726205\n",
            "triplet loss: 0.14156520853822047\n",
            "cross entropy loss: 0.3957487500283713\n",
            "margin: 0.002 lambda: 0.2\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(79.5917)\n",
            "total loss: 0.6784032039932096\n",
            "triplet loss: tensor(0.0835, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2902, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.9630)\n",
            "total loss: 0.38599901214330157\n",
            "triplet loss: 0.14905753363607196\n",
            "cross entropy loss: 0.35618750587899306\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(91.3783)\n",
            "total loss: 0.29559697197285545\n",
            "triplet loss: tensor(0.0508, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2338, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.9028)\n",
            "total loss: 0.32352962818812336\n",
            "triplet loss: 0.1218983494075974\n",
            "cross entropy loss: 0.2991499577004176\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(93.8517)\n",
            "total loss: 0.21333365052588968\n",
            "triplet loss: tensor(0.0403, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1794, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.7685)\n",
            "total loss: 0.2942419893290164\n",
            "triplet loss: 0.08442579138715416\n",
            "cross entropy loss: 0.2773568310593007\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(95.7667)\n",
            "total loss: 0.15518519463045383\n",
            "triplet loss: tensor(0.0844, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0436, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9815)\n",
            "total loss: 0.2953103620051985\n",
            "triplet loss: 0.11070984723211745\n",
            "cross entropy loss: 0.2731683927659805\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(96.6933)\n",
            "total loss: 0.12173951870755854\n",
            "triplet loss: tensor(0.0808, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1109, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1713)\n",
            "total loss: 0.30092655398921503\n",
            "triplet loss: 0.0999738528674259\n",
            "cross entropy loss: 0.2809317828591406\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.5517)\n",
            "total loss: 0.09485174757418538\n",
            "triplet loss: tensor(0.0976, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0184, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.2593)\n",
            "total loss: 0.3103354192675042\n",
            "triplet loss: 0.10827582945617345\n",
            "cross entropy loss: 0.28868025298831024\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(98.0083)\n",
            "total loss: 0.07940263366564981\n",
            "triplet loss: tensor(0.0203, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0430, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1898)\n",
            "total loss: 0.33830694960066554\n",
            "triplet loss: 0.10997127397151212\n",
            "cross entropy loss: 0.3163126944591837\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.4450)\n",
            "total loss: 0.0641666096307512\n",
            "triplet loss: tensor(0.0357, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0128, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9398)\n",
            "total loss: 0.3675764146200299\n",
            "triplet loss: 0.11709237263190764\n",
            "cross entropy loss: 0.34415794047113707\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.6567)\n",
            "total loss: 0.054009188790676566\n",
            "triplet loss: tensor(0.0403, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0099, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3380)\n",
            "total loss: 0.36868590179323796\n",
            "triplet loss: 0.11469734633705143\n",
            "cross entropy loss: 0.34574643248339465\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(98.9100)\n",
            "total loss: 0.04563116432646158\n",
            "triplet loss: tensor(0.0338, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2254, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1435)\n",
            "total loss: 0.38244562583958963\n",
            "triplet loss: 0.12660106006436622\n",
            "cross entropy loss: 0.3571254140008572\n",
            "margin: 0.002 lambda: 0.4\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(78.6800)\n",
            "total loss: 0.7376629573894716\n",
            "triplet loss: tensor(0.0952, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.6396, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.7500)\n",
            "total loss: 0.40918915546857393\n",
            "triplet loss: 0.09921854616489989\n",
            "cross entropy loss: 0.36950173674250497\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(91.2650)\n",
            "total loss: 0.314693950537616\n",
            "triplet loss: tensor(0.0418, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1955, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.3009)\n",
            "total loss: 0.32251158477903824\n",
            "triplet loss: 0.07601537070507129\n",
            "cross entropy loss: 0.29210543646835363\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(93.8783)\n",
            "total loss: 0.22287846004753226\n",
            "triplet loss: tensor(0.0382, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3970, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.5602)\n",
            "total loss: 0.30723734916402745\n",
            "triplet loss: 0.06561828139045182\n",
            "cross entropy loss: 0.2809900358407455\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(95.5750)\n",
            "total loss: 0.1674347614238003\n",
            "triplet loss: tensor(0.0068, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3097, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9167)\n",
            "total loss: 0.3129706145016223\n",
            "triplet loss: 0.08159094527584208\n",
            "cross entropy loss: 0.280334235538216\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(96.8483)\n",
            "total loss: 0.12540560323737068\n",
            "triplet loss: tensor(0.0289, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2189, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.8704)\n",
            "total loss: 0.32126143863479767\n",
            "triplet loss: 0.08582577400306272\n",
            "cross entropy loss: 0.2869311290374301\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.5467)\n",
            "total loss: 0.1001687680176501\n",
            "triplet loss: tensor(0.0163, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0045, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9352)\n",
            "total loss: 0.33907667791041046\n",
            "triplet loss: 0.08442666098894276\n",
            "cross entropy loss: 0.30530601433483806\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(98.0783)\n",
            "total loss: 0.08047822781347994\n",
            "triplet loss: tensor(0.0469, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2956, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.2130)\n",
            "total loss: 0.34874697403221794\n",
            "triplet loss: 0.09732458968343731\n",
            "cross entropy loss: 0.3098171376044581\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.5033)\n",
            "total loss: 0.06505024126137117\n",
            "triplet loss: tensor(0.0099, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1815, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0139)\n",
            "total loss: 0.39717177497829204\n",
            "triplet loss: 0.1150035922685583\n",
            "cross entropy loss: 0.3511703381171593\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.7100)\n",
            "total loss: 0.05667470177627607\n",
            "triplet loss: tensor(0.0027, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0028, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.2176)\n",
            "total loss: 0.4134376085853788\n",
            "triplet loss: 0.11484866767030584\n",
            "cross entropy loss: 0.36749814091437666\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(98.8783)\n",
            "total loss: 0.048237944655651566\n",
            "triplet loss: tensor(0.0097, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0211, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9907)\n",
            "total loss: 0.41879734551236475\n",
            "triplet loss: 0.12656823895749714\n",
            "cross entropy loss: 0.36817004889547383\n",
            "margin: 0.004 lambda: 0.1\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(79.4133)\n",
            "total loss: 0.6691123641598453\n",
            "triplet loss: tensor(0.1311, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1518, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.1852)\n",
            "total loss: 0.38392816122466994\n",
            "triplet loss: 0.20235940276165687\n",
            "cross entropy loss: 0.36369222082947134\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(91.1267)\n",
            "total loss: 0.29464440349576826\n",
            "triplet loss: tensor(0.1087, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1170, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.8889)\n",
            "total loss: 0.31310791771351937\n",
            "triplet loss: 0.1235340025342075\n",
            "cross entropy loss: 0.3007545179340261\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(93.9000)\n",
            "total loss: 0.20773089559538277\n",
            "triplet loss: tensor(0.0495, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3075, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.2870)\n",
            "total loss: 0.30866468287783966\n",
            "triplet loss: 0.10102802275861861\n",
            "cross entropy loss: 0.2985618798139356\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(95.4383)\n",
            "total loss: 0.1556808303005056\n",
            "triplet loss: tensor(0.0616, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2032, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.6019)\n",
            "total loss: 0.30450981657004217\n",
            "triplet loss: 0.12093327208780326\n",
            "cross entropy loss: 0.2924164891705887\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(96.6350)\n",
            "total loss: 0.11911717850937327\n",
            "triplet loss: tensor(0.1777, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1205, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.8935)\n",
            "total loss: 0.3197598086570847\n",
            "triplet loss: 0.14448024307173737\n",
            "cross entropy loss: 0.30531178460561137\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.3567)\n",
            "total loss: 0.09433927414899092\n",
            "triplet loss: tensor(0.1226, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4027, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.8380)\n",
            "total loss: 0.33344810386601637\n",
            "triplet loss: 0.11293563440706603\n",
            "cross entropy loss: 0.322154541385564\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(98.0667)\n",
            "total loss: 0.073243861893879\n",
            "triplet loss: tensor(0.0729, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0058, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.4444)\n",
            "total loss: 0.36566635045999957\n",
            "triplet loss: 0.12485874414488056\n",
            "cross entropy loss: 0.3531804761943027\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.3433)\n",
            "total loss: 0.06346246769635884\n",
            "triplet loss: tensor(0.0807, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0182, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0046)\n",
            "total loss: 0.3741303443710303\n",
            "triplet loss: 0.12287127039224438\n",
            "cross entropy loss: 0.36184321751972365\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.6883)\n",
            "total loss: 0.054813241867074515\n",
            "triplet loss: tensor(0.1302, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0683, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1111)\n",
            "total loss: 0.4005297511904197\n",
            "triplet loss: 0.12774959196661703\n",
            "cross entropy loss: 0.38775479203894647\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(98.7450)\n",
            "total loss: 0.05240915752331744\n",
            "triplet loss: tensor(0.0821, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0138, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.4954)\n",
            "total loss: 0.4223417528910485\n",
            "triplet loss: 0.13406241504398322\n",
            "cross entropy loss: 0.40893551159771324\n",
            "margin: 0.004 lambda: 0.2\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(79.3717)\n",
            "total loss: 0.6854406282750528\n",
            "triplet loss: tensor(0.0673, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.5338, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.9907)\n",
            "total loss: 0.39583185910296864\n",
            "triplet loss: 0.12924768645647008\n",
            "cross entropy loss: 0.36998232253468955\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(91.4500)\n",
            "total loss: 0.2969618993702092\n",
            "triplet loss: tensor(0.0547, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2211, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.9444)\n",
            "total loss: 0.3174864233818633\n",
            "triplet loss: 0.09959853638911388\n",
            "cross entropy loss: 0.29756671582629696\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(94.1517)\n",
            "total loss: 0.20340559462001964\n",
            "triplet loss: tensor(0.0624, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0532, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.8935)\n",
            "total loss: 0.2977571657379351\n",
            "triplet loss: 0.09676778589832712\n",
            "cross entropy loss: 0.27840360848111867\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(95.8217)\n",
            "total loss: 0.1525696825061335\n",
            "triplet loss: tensor(0.1183, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0290, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.2870)\n",
            "total loss: 0.29073006086624587\n",
            "triplet loss: 0.0965328180512204\n",
            "cross entropy loss: 0.27142349778283276\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(96.8667)\n",
            "total loss: 0.11573400712812315\n",
            "triplet loss: tensor(0.0717, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0311, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1204)\n",
            "total loss: 0.3204754478347725\n",
            "triplet loss: 0.1096387443810525\n",
            "cross entropy loss: 0.29854769782664686\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.6717)\n",
            "total loss: 0.09119314109302883\n",
            "triplet loss: tensor(0.0256, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0575, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9907)\n",
            "total loss: 0.3316893720745864\n",
            "triplet loss: 0.09904292325031828\n",
            "cross entropy loss: 0.31188078738539615\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(98.2517)\n",
            "total loss: 0.0714285501511271\n",
            "triplet loss: tensor(0.1441, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0390, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3704)\n",
            "total loss: 0.3561297078016242\n",
            "triplet loss: 0.11621614324876807\n",
            "cross entropy loss: 0.33288647905653396\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.6250)\n",
            "total loss: 0.05938083958327532\n",
            "triplet loss: tensor(0.1112, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1731, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.2778)\n",
            "total loss: 0.3711678038555137\n",
            "triplet loss: 0.12747906543005852\n",
            "cross entropy loss: 0.34567199030769824\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.7467)\n",
            "total loss: 0.05477167834152481\n",
            "triplet loss: tensor(0.0425, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0030, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.2593)\n",
            "total loss: 0.3988939037425278\n",
            "triplet loss: 0.12991039776766794\n",
            "cross entropy loss: 0.37291182329624717\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(99.0100)\n",
            "total loss: 0.0442757625510094\n",
            "triplet loss: tensor(0.0074, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1269, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.6528)\n",
            "total loss: 0.4183694090054938\n",
            "triplet loss: 0.1177965512708623\n",
            "cross entropy loss: 0.3948100971983823\n",
            "margin: 0.004 lambda: 0.4\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(79.0417)\n",
            "total loss: 0.7268521891696367\n",
            "triplet loss: tensor(0.0437, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4690, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.5556)\n",
            "total loss: 0.39003074645290714\n",
            "triplet loss: 0.11539726710971995\n",
            "cross entropy loss: 0.34387183936654464\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(91.2150)\n",
            "total loss: 0.3075546640148168\n",
            "triplet loss: tensor(0.0526, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2271, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.8333)\n",
            "total loss: 0.34032746688765886\n",
            "triplet loss: 0.10527111911738413\n",
            "cross entropy loss: 0.2982190185000558\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(94.0500)\n",
            "total loss: 0.21431585268647685\n",
            "triplet loss: tensor(0.0736, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0765, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.5278)\n",
            "total loss: 0.31028530801806226\n",
            "triplet loss: 0.08591672446380352\n",
            "cross entropy loss: 0.2759186188420336\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(95.8183)\n",
            "total loss: 0.15811227022338586\n",
            "triplet loss: tensor(0.0403, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1139, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.7361)\n",
            "total loss: 0.3054502150760247\n",
            "triplet loss: 0.0706451760312906\n",
            "cross entropy loss: 0.27719214399890785\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(96.9133)\n",
            "total loss: 0.12057932740700905\n",
            "triplet loss: tensor(0.0029, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0867, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0833)\n",
            "total loss: 0.32449848059366443\n",
            "triplet loss: 0.09104298689463258\n",
            "cross entropy loss: 0.2880812857228403\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.6533)\n",
            "total loss: 0.09509726798237324\n",
            "triplet loss: tensor(0.0109, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0543, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.8796)\n",
            "total loss: 0.3460284285577973\n",
            "triplet loss: 0.09061952387726695\n",
            "cross entropy loss: 0.3097806180436054\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(98.1450)\n",
            "total loss: 0.0766870003652328\n",
            "triplet loss: tensor(0.0635, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2268, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.4167)\n",
            "total loss: 0.35661817493038417\n",
            "triplet loss: 0.10890228306797482\n",
            "cross entropy loss: 0.31305726119262006\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.5183)\n",
            "total loss: 0.06269135619469011\n",
            "triplet loss: tensor(0.0276, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0082, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.5463)\n",
            "total loss: 0.34530799898743275\n",
            "triplet loss: 0.09650025625655306\n",
            "cross entropy loss: 0.30670789657325964\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.8333)\n",
            "total loss: 0.05064235202871811\n",
            "triplet loss: tensor(0.0063, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0119, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.2917)\n",
            "total loss: 0.38164823904589434\n",
            "triplet loss: 0.11095615704137468\n",
            "cross entropy loss: 0.33726577564919663\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(98.9683)\n",
            "total loss: 0.04453314232777979\n",
            "triplet loss: tensor(0., grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0018, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.8981)\n",
            "total loss: 0.45865696033782505\n",
            "triplet loss: 0.11835425197403085\n",
            "cross entropy loss: 0.4113152584938619\n",
            "margin: 0.008 lambda: 0.1\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(79.5617)\n",
            "total loss: 0.6732052242689168\n",
            "triplet loss: tensor(0.1080, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4033, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.3056)\n",
            "total loss: 0.3804836658507409\n",
            "triplet loss: 0.17606780532549118\n",
            "cross entropy loss: 0.3628768856091612\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(91.5450)\n",
            "total loss: 0.2851901586487222\n",
            "triplet loss: tensor(0.1682, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1376, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.1620)\n",
            "total loss: 0.31745980955528086\n",
            "triplet loss: 0.19755916079208696\n",
            "cross entropy loss: 0.2977038941417749\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(94.1183)\n",
            "total loss: 0.1964760305348045\n",
            "triplet loss: tensor(0.0589, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0994, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.8750)\n",
            "total loss: 0.29030107847861286\n",
            "triplet loss: 0.1283195614924974\n",
            "cross entropy loss: 0.2774691225288535\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(95.8600)\n",
            "total loss: 0.14354540778200892\n",
            "triplet loss: tensor(0.1304, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1478, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.8565)\n",
            "total loss: 0.3012137329371194\n",
            "triplet loss: 0.14308705544595182\n",
            "cross entropy loss: 0.286905027418976\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(97.0350)\n",
            "total loss: 0.10768680826131342\n",
            "triplet loss: tensor(0.1300, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0223, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3796)\n",
            "total loss: 0.30260475688564353\n",
            "triplet loss: 0.13123805409837405\n",
            "cross entropy loss: 0.28948095134464946\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.7117)\n",
            "total loss: 0.08509525322099167\n",
            "triplet loss: tensor(0.0760, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0213, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9491)\n",
            "total loss: 0.3361358210532623\n",
            "triplet loss: 0.1720547718850113\n",
            "cross entropy loss: 0.3189303434988451\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(98.1617)\n",
            "total loss: 0.0690979705448312\n",
            "triplet loss: tensor(0.0437, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0243, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.2546)\n",
            "total loss: 0.33764294670486944\n",
            "triplet loss: 0.12248889215775495\n",
            "cross entropy loss: 0.3253940576081269\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.6400)\n",
            "total loss: 0.05549991260002703\n",
            "triplet loss: tensor(0.0636, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0226, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1944)\n",
            "total loss: 0.3523510628925449\n",
            "triplet loss: 0.12902933160757876\n",
            "cross entropy loss: 0.33944812977752653\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.9400)\n",
            "total loss: 0.04717137158435307\n",
            "triplet loss: tensor(0.0517, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0086, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9676)\n",
            "total loss: 0.3871867567873918\n",
            "triplet loss: 0.13040468529880753\n",
            "cross entropy loss: 0.3741462875637033\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(98.7650)\n",
            "total loss: 0.049413691772751685\n",
            "triplet loss: tensor(0.0825, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0578, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3889)\n",
            "total loss: 0.41026993519103033\n",
            "triplet loss: 0.17294556709259926\n",
            "cross entropy loss: 0.3929753782084355\n",
            "margin: 0.008 lambda: 0.2\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(79.7567)\n",
            "total loss: 0.6704665427046544\n",
            "triplet loss: tensor(0.0717, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3249, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.7917)\n",
            "total loss: 0.3649682845236987\n",
            "triplet loss: 0.13283687523571697\n",
            "cross entropy loss: 0.33840090929139294\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(91.8550)\n",
            "total loss: 0.2866672249252735\n",
            "triplet loss: tensor(0.0927, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0831, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.4306)\n",
            "total loss: 0.29586839019017813\n",
            "triplet loss: 0.11013611778616905\n",
            "cross entropy loss: 0.2738411663618137\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(94.3517)\n",
            "total loss: 0.19872701308652282\n",
            "triplet loss: tensor(0.0819, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0457, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0833)\n",
            "total loss: 0.2883733660203113\n",
            "triplet loss: 0.1082642939073976\n",
            "cross entropy loss: 0.2667205073710906\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(96.0867)\n",
            "total loss: 0.14388066212505674\n",
            "triplet loss: tensor(0.0391, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1672, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3750)\n",
            "total loss: 0.29754209353109085\n",
            "triplet loss: 0.11479843079426585\n",
            "cross entropy loss: 0.27458240757503455\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(97.0967)\n",
            "total loss: 0.10863820488936007\n",
            "triplet loss: tensor(0.0580, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1232, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1852)\n",
            "total loss: 0.3160687567633108\n",
            "triplet loss: 0.11974787526383732\n",
            "cross entropy loss: 0.2921191818306785\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.8133)\n",
            "total loss: 0.08644703143377548\n",
            "triplet loss: tensor(0.0070, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0752, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.5139)\n",
            "total loss: 0.3097506462976601\n",
            "triplet loss: 0.11109337684582676\n",
            "cross entropy loss: 0.28753197102768885\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(98.3383)\n",
            "total loss: 0.06975934918481372\n",
            "triplet loss: tensor(0.0460, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1102, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1435)\n",
            "total loss: 0.3497256390177287\n",
            "triplet loss: 0.11724357147695574\n",
            "cross entropy loss: 0.3262769238990263\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.6400)\n",
            "total loss: 0.056196484501773454\n",
            "triplet loss: tensor(0.0681, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0174, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1343)\n",
            "total loss: 0.3736960645324203\n",
            "triplet loss: 0.13394955218323237\n",
            "cross entropy loss: 0.34690615420433635\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.8167)\n",
            "total loss: 0.050812843786890126\n",
            "triplet loss: tensor(0.0572, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0755, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3565)\n",
            "total loss: 0.402872534623072\n",
            "triplet loss: 0.14547646215968055\n",
            "cross entropy loss: 0.3737772423470911\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(99.0800)\n",
            "total loss: 0.04151103630744374\n",
            "triplet loss: tensor(0.0346, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0308, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1389)\n",
            "total loss: 0.4029666170610126\n",
            "triplet loss: 0.12330156546802475\n",
            "cross entropy loss: 0.3783063034350659\n",
            "margin: 0.008 lambda: 0.4\n",
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(79.0450)\n",
            "total loss: 0.7289961678768272\n",
            "triplet loss: tensor(0.0576, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.5921, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.7639)\n",
            "total loss: 0.4041083043319939\n",
            "triplet loss: 0.1052633892502305\n",
            "cross entropy loss: 0.3620029491697543\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(91.5867)\n",
            "total loss: 0.3027327389780011\n",
            "triplet loss: tensor(0.0491, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1467, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.4259)\n",
            "total loss: 0.31061118464233606\n",
            "triplet loss: 0.07637691341296456\n",
            "cross entropy loss: 0.280060418776771\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(94.3217)\n",
            "total loss: 0.21294895926121074\n",
            "triplet loss: tensor(0.0320, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0443, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9722)\n",
            "total loss: 0.2959788755001225\n",
            "triplet loss: 0.08047062904744812\n",
            "cross entropy loss: 0.2637906236540989\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(95.9183)\n",
            "total loss: 0.15554694467579633\n",
            "triplet loss: tensor(0.0497, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0365, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3519)\n",
            "total loss: 0.29023215328976953\n",
            "triplet loss: 0.09032916960984293\n",
            "cross entropy loss: 0.2541004847955598\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(96.9750)\n",
            "total loss: 0.11960930806368208\n",
            "triplet loss: tensor(0.0045, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0465, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.8611)\n",
            "total loss: 0.30238124188145943\n",
            "triplet loss: 0.08890115876114492\n",
            "cross entropy loss: 0.2668207779681013\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.8217)\n",
            "total loss: 0.08963209341194775\n",
            "triplet loss: tensor(0.0120, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1576, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.5694)\n",
            "total loss: 0.3243771787931228\n",
            "triplet loss: 0.09606634078756768\n",
            "cross entropy loss: 0.2859506419468561\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(98.3033)\n",
            "total loss: 0.07219507068054064\n",
            "triplet loss: tensor(0.0145, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0155, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.4259)\n",
            "total loss: 0.3429934165975046\n",
            "triplet loss: 0.10788550845088338\n",
            "cross entropy loss: 0.29983921235526456\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.7250)\n",
            "total loss: 0.057604176468396585\n",
            "triplet loss: tensor(0.0559, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0107, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1991)\n",
            "total loss: 0.3815663263267843\n",
            "triplet loss: 0.11813017096337394\n",
            "cross entropy loss: 0.3343142563333878\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.8667)\n",
            "total loss: 0.05012065459038816\n",
            "triplet loss: tensor(0.0084, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0028, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0648)\n",
            "total loss: 0.4253644737740152\n",
            "triplet loss: 0.13620713967058432\n",
            "cross entropy loss: 0.37088161653745366\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(99.0133)\n",
            "total loss: 0.043939735363873\n",
            "triplet loss: tensor(0.0069, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0744, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.7824)\n",
            "total loss: 0.4134816471377068\n",
            "triplet loss: 0.13639972304109726\n",
            "cross entropy loss: 0.3589217562955865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fit(0.005, 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O0Tm9mKHz-C",
        "outputId": "b8352558-93ab-40e3-e127-f6c0a7704a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(79.0133)\n",
            "total loss: 0.6988773735315561\n",
            "triplet loss: tensor(0.0815, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1923, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(88.2639)\n",
            "total loss: 0.4106061971311033\n",
            "triplet loss: 0.12784628121501948\n",
            "cross entropy loss: 0.3850369403612684\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(91.6117)\n",
            "total loss: 0.2946008112289504\n",
            "triplet loss: tensor(0.0946, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2094, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.7407)\n",
            "total loss: 0.33567441091734984\n",
            "triplet loss: 0.12679143700770726\n",
            "cross entropy loss: 0.3103161233328503\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(94.0467)\n",
            "total loss: 0.20669875517169803\n",
            "triplet loss: tensor(0.0208, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2418, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.8611)\n",
            "total loss: 0.2942581290941267\n",
            "triplet loss: 0.09962875951500334\n",
            "cross entropy loss: 0.27433237740935307\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(95.8217)\n",
            "total loss: 0.1512725889555681\n",
            "triplet loss: tensor(0.0420, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1478, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1667)\n",
            "total loss: 0.29306914832597303\n",
            "triplet loss: 0.0992203529802917\n",
            "cross entropy loss: 0.273225077675909\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(96.8750)\n",
            "total loss: 0.11558435274871873\n",
            "triplet loss: tensor(0.1031, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1346, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9491)\n",
            "total loss: 0.3088448814739137\n",
            "triplet loss: 0.08950071768096741\n",
            "cross entropy loss: 0.29094473758613215\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(97.6917)\n",
            "total loss: 0.09024502139594127\n",
            "triplet loss: tensor(0.0507, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0110, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0741)\n",
            "total loss: 0.3246332125617026\n",
            "triplet loss: 0.12665143012780056\n",
            "cross entropy loss: 0.29930292628154426\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(98.1583)\n",
            "total loss: 0.07432931431158106\n",
            "triplet loss: tensor(0.0727, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0874, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0741)\n",
            "total loss: 0.34601879636784627\n",
            "triplet loss: 0.11057122065492812\n",
            "cross entropy loss: 0.32390455237573246\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(98.4200)\n",
            "total loss: 0.06448417080880038\n",
            "triplet loss: tensor(0.0013, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0573, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1435)\n",
            "total loss: 0.359277928832191\n",
            "triplet loss: 0.12165996733744293\n",
            "cross entropy loss: 0.3349459348687318\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(98.8833)\n",
            "total loss: 0.049934400391500836\n",
            "triplet loss: tensor(0.0010, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0067, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.8148)\n",
            "total loss: 0.39997691636872007\n",
            "triplet loss: 0.1484158321272736\n",
            "cross entropy loss: 0.37029374931999565\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(98.8983)\n",
            "total loss: 0.04789347376730015\n",
            "triplet loss: tensor(0.0587, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0097, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.7407)\n",
            "total loss: 0.4250372809594905\n",
            "triplet loss: 0.1545334802954265\n",
            "cross entropy loss: 0.3941305842823707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.fc_img = nn.Linear(32 * 8 * 8, 256)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.fc_combined = nn.Linear(512, 256)# Concatenated feature and image size\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.fc_domain = nn.Linear(256, 5)\n",
        "        self.fc_combined2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x_img, x_features):\n",
        "        x_img = self.conv1(x_img)\n",
        "        x_img = self.conv2(x_img)\n",
        "        x_img = x_img.view(x_img.size(0), -1)  # Flatten the image\n",
        "        x_img = F.relu(self.fc_img(x_img))\n",
        "        x_img = self.dropout1(x_img)\n",
        "\n",
        "        # Concatenate features and image representations\n",
        "        x_combined = torch.cat((x_img, x_features), dim=1)\n",
        "\n",
        "        # Fully connected layers for prediction\n",
        "        x_combined = F.relu(self.fc_combined(x_combined))\n",
        "        x_combined = self.dropout2(x_combined)\n",
        "        x_combined2 = self.fc_combined2(x_combined)\n",
        "        x_combined = self.fc_domain(x_combined)\n",
        "        return x_combined2, x_combined\n",
        "\n",
        "def fit(margin, lambd):\n",
        "  model = CombinedModel()\n",
        "  crossLoss = nn.CrossEntropyLoss()\n",
        "  triplet = triplet_loss(margin=margin)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  epochs = 10\n",
        "  for epoch in range(epochs):\n",
        "    print('epoch: ' + str(epoch))\n",
        "    model.train()\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    triplet_total = 0.0\n",
        "    entropy_total = 0.0\n",
        "\n",
        "    num_samples = len(train.dataset)\n",
        "    num_batches = len(train)\n",
        "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(train):\n",
        "\n",
        "      outs2, outs = model(images, features)\n",
        "\n",
        "      loss1 = crossLoss(outs2, digit_labels)\n",
        "      loss2 = triplet(outs, domain_labels)\n",
        "      total_loss = loss1 + lambd * loss2\n",
        "\n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      _, preds = torch.max(outs2, dim=1) # Explain, [N]\n",
        "      running_corrects += torch.sum(preds == digit_labels)\n",
        "      running_loss += total_loss.item()\n",
        "      triplet_total += loss2.item()\n",
        "      entropy_total += loss1.item()\n",
        "\n",
        "    epoch_acc = (running_corrects / num_samples) * 100\n",
        "    epoch_loss = (running_loss / num_batches)\n",
        "    triplet_total  = (triplet_total / num_batches)\n",
        "    entropy_total = (entropy_total / num_batches)\n",
        "    print('train')\n",
        "    print('accuaray: ' + str(epoch_acc))\n",
        "    print('total loss: ' + str(epoch_loss))\n",
        "    print('triplet loss: ' + str(loss2))\n",
        "    print('cross entropy loss: ' + str(loss1))\n",
        "    print('---------------')\n",
        "\n",
        "\n",
        "    num_samples_test = len(test.dataset)\n",
        "    num_batches_test = len(test)\n",
        "    running_corrects_test = 0\n",
        "    running_loss_test = 0.0\n",
        "    triplet_total = 0.0\n",
        "    entropy_total = 0.0\n",
        "\n",
        "\n",
        "    model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "    with torch.no_grad(): # explain\n",
        "        # more on torch.no_grad(): https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#disabling-gradient-tracking\n",
        "\n",
        "        for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(test): # Get a batch of Data\n",
        "\n",
        "            outputs2, outputs = model(images, features) # Forward Pass\n",
        "            loss = crossLoss(outputs2, digit_labels) # Compute Loss\n",
        "            loss2 = triplet(outputs, domain_labels)\n",
        "            total_loss = loss + lambd * loss2\n",
        "\n",
        "            _, preds = torch.max(outputs2, 1) #\n",
        "            running_corrects_test += torch.sum(preds == digit_labels)\n",
        "            running_loss_test += total_loss.item()\n",
        "            triplet_total += loss2.item()\n",
        "            entropy_total += loss.item()\n",
        "\n",
        "    test_acc = (running_corrects_test / num_samples_test) * 100\n",
        "    test_loss = (running_loss_test / num_batches_test)\n",
        "    triplet_total = (triplet_total / num_batches_test)\n",
        "    entropy_total = (entropy_total / num_batches_test)\n",
        "    print('test')\n",
        "    print('accuray: ' + str(test_acc))\n",
        "    print('total loss: ' + str(test_loss))\n",
        "    print('triplet loss: ' + str(triplet_total))\n",
        "    print('cross entropy loss: ' + str(entropy_total))\n",
        "\n",
        "fit(0.008, 0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-Cku1cJJVKm",
        "outputId": "a3517e75-4152-4412-898e-f3159d951dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(76.4100)\n",
            "total loss: 0.7636653595228693\n",
            "triplet loss: tensor(0.0745, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2547, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.0741)\n",
            "total loss: 0.38581037997494083\n",
            "triplet loss: 0.11705151623522742\n",
            "cross entropy loss: 0.368252651754921\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(88.9983)\n",
            "total loss: 0.3702047910453922\n",
            "triplet loss: tensor(0.0961, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1058, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.7593)\n",
            "total loss: 0.3159794926246595\n",
            "triplet loss: 0.09530411878898298\n",
            "cross entropy loss: 0.30168387337901886\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(91.4783)\n",
            "total loss: 0.28598173221608975\n",
            "triplet loss: tensor(0.1094, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1693, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.5324)\n",
            "total loss: 0.2956723269493975\n",
            "triplet loss: 0.10039115423810553\n",
            "cross entropy loss: 0.28061365375857383\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(92.8550)\n",
            "total loss: 0.23860661264882285\n",
            "triplet loss: tensor(0.0844, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1060, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.0185)\n",
            "total loss: 0.2818928348705084\n",
            "triplet loss: 0.08097526961221145\n",
            "cross entropy loss: 0.2697465438285523\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(93.9417)\n",
            "total loss: 0.20532890099849402\n",
            "triplet loss: tensor(0.0816, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1256, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.4630)\n",
            "total loss: 0.2732758799115934\n",
            "triplet loss: 0.08726254453849511\n",
            "cross entropy loss: 0.2601864977061925\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(94.7617)\n",
            "total loss: 0.18206224680693547\n",
            "triplet loss: tensor(0.0917, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1370, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.7269)\n",
            "total loss: 0.271671046662701\n",
            "triplet loss: 0.10580174404488513\n",
            "cross entropy loss: 0.25580078452968824\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(95.2850)\n",
            "total loss: 0.16211645794051416\n",
            "triplet loss: tensor(0.0902, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3219, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.8796)\n",
            "total loss: 0.2725355955799656\n",
            "triplet loss: 0.08848510296827942\n",
            "cross entropy loss: 0.2592628295395589\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(95.7500)\n",
            "total loss: 0.14729739117946453\n",
            "triplet loss: tensor(0.0813, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1680, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.6111)\n",
            "total loss: 0.27661465278126784\n",
            "triplet loss: 0.083116972631604\n",
            "cross entropy loss: 0.26414710621641585\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(96.2050)\n",
            "total loss: 0.13491675128608244\n",
            "triplet loss: tensor(0.1093, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3817, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.9213)\n",
            "total loss: 0.29356064305767504\n",
            "triplet loss: 0.10130163315220697\n",
            "cross entropy loss: 0.2783653965803998\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(96.4400)\n",
            "total loss: 0.12751464161084597\n",
            "triplet loss: tensor(0.1012, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1391, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.8426)\n",
            "total loss: 0.29207578428177794\n",
            "triplet loss: 0.09377580252889345\n",
            "cross entropy loss: 0.27800941297552995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Number Of Parameteres**"
      ],
      "metadata": {
        "id": "70gO46tOe1o4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = CombinedModel()\n",
        "def num_params():\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(num_params())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tluooJFSW3Sx",
        "outputId": "26e3f6e5-5697-4a24-d0de-ec36d52bef88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "663530\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extra point**"
      ],
      "metadata": {
        "id": "ajXCy0Q_e7og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extra Point\n",
        "\n",
        "# 1: augmentation\n",
        "\n",
        "# 2: duplicate image and concat it with itself (Selected)\n"
      ],
      "metadata": {
        "id": "nM8DODIPQg6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Before Handling Test Missing**"
      ],
      "metadata": {
        "id": "YdsC7wmFfEJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.fc_img = nn.Linear(32 * 8 * 8, 256)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.fc_combined = nn.Linear(512, 256)# Concatenated feature and image size\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.fc_domain = nn.Linear(256, 5)\n",
        "        self.fc_combined2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x_img, x_features):\n",
        "        x_img = self.conv1(x_img)\n",
        "        x_img = self.conv2(x_img)\n",
        "        x_img = x_img.view(x_img.size(0), -1)  # Flatten the image\n",
        "        x_img = F.relu(self.fc_img(x_img))\n",
        "        x_img = self.dropout1(x_img)\n",
        "\n",
        "        # Concatenate features and image representations\n",
        "        x_combined = torch.cat((x_img, x_features), dim=1)\n",
        "\n",
        "        # Fully connected layers for prediction\n",
        "        x_combined = F.relu(self.fc_combined(x_combined))\n",
        "        x_combined = self.dropout2(x_combined)\n",
        "        x_combined2 = self.fc_combined2(x_combined)\n",
        "        x_combined = self.fc_domain(x_combined)\n",
        "        return x_combined2, x_combined\n",
        "\n",
        "model = CombinedModel()\n",
        "crossLoss = nn.CrossEntropyLoss()\n",
        "triplet = triplet_loss(margin=0.008)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  print('epoch: ' + str(epoch))\n",
        "  model.train()\n",
        "  running_corrects = 0\n",
        "  running_loss = 0.0\n",
        "  triplet_total = 0.0\n",
        "  entropy_total = 0.0\n",
        "\n",
        "  num_samples = len(train.dataset)\n",
        "  num_batches = len(train)\n",
        "  for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(train):\n",
        "\n",
        "    outs2, outs = model(images, features)\n",
        "\n",
        "    loss1 = crossLoss(outs2, digit_labels)\n",
        "    loss2 = triplet(outs, domain_labels)\n",
        "    total_loss = loss1 + 0.4 * loss2\n",
        "\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    _, preds = torch.max(outs2, dim=1) # Explain, [N]\n",
        "    running_corrects += torch.sum(preds == digit_labels)\n",
        "    running_loss += total_loss.item()\n",
        "    triplet_total += loss2.item()\n",
        "    entropy_total += loss1.item()\n",
        "\n",
        "  epoch_acc = (running_corrects / num_samples) * 100\n",
        "  epoch_loss = (running_loss / num_batches)\n",
        "  triplet_total  = (triplet_total / num_batches)\n",
        "  entropy_total = (entropy_total / num_batches)\n",
        "  print('train')\n",
        "  print('accuaray: ' + str(epoch_acc))\n",
        "  print('total loss: ' + str(epoch_loss))\n",
        "  print('triplet loss: ' + str(loss2))\n",
        "  print('cross entropy loss: ' + str(loss1))\n",
        "  print('---------------')\n",
        "\n",
        "\n",
        "  num_samples_test = len(test.dataset)\n",
        "  num_batches_test = len(test)\n",
        "  running_corrects_test = 0\n",
        "  running_loss_test = 0.0\n",
        "  triplet_total = 0.0\n",
        "  entropy_total = 0.0\n",
        "\n",
        "  model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "  with torch.no_grad(): # explain\n",
        "      # more on torch.no_grad(): https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#disabling-gradient-tracking\n",
        "\n",
        "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(missing): # Get a batch of Data\n",
        "\n",
        "        outputs2, outputs = model(images, features) # Forward Pass\n",
        "        loss = crossLoss(outputs2, digit_labels) # Compute Loss\n",
        "        loss2 = triplet(outputs, domain_labels)\n",
        "        total_loss = loss + 0.4 * loss2\n",
        "\n",
        "        _, preds = torch.max(outputs2, 1) #\n",
        "        running_corrects_test += torch.sum(preds == digit_labels)\n",
        "        running_loss_test += total_loss.item()\n",
        "        triplet_total += loss2.item()\n",
        "        entropy_total += loss.item()\n",
        "\n",
        "    test_acc = (running_corrects_test / num_samples_test) * 100\n",
        "    test_loss = (running_loss_test / num_batches_test)\n",
        "    triplet_total = (triplet_total / num_batches_test)\n",
        "    entropy_total = (entropy_total / num_batches_test)\n",
        "    print('test')\n",
        "    print('accuray: ' + str(test_acc))\n",
        "    print('total loss: ' + str(test_loss))\n",
        "    print('triplet loss: ' + str(triplet_total))\n",
        "    print('cross entropy loss: ' + str(entropy_total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui1GO1wmaQBC",
        "outputId": "e532de28-e5fa-4aa5-9a8d-ec9cf8ddd75a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(76.8900)\n",
            "total loss: 0.7511226581866299\n",
            "triplet loss: tensor(0.1111, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.4785, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(85.6620)\n",
            "total loss: 0.4721518209611876\n",
            "triplet loss: 0.09376473357823827\n",
            "cross entropy loss: 0.4580871100873637\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(89.0167)\n",
            "total loss: 0.36786007908965224\n",
            "triplet loss: tensor(0.0901, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3898, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(87.8194)\n",
            "total loss: 0.4182518363087135\n",
            "triplet loss: 0.06734076522555224\n",
            "cross entropy loss: 0.4081507212752421\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(91.7150)\n",
            "total loss: 0.2780628157878863\n",
            "triplet loss: tensor(0.0814, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2384, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.7454)\n",
            "total loss: 0.3759925733347969\n",
            "triplet loss: 0.06060243591930739\n",
            "cross entropy loss: 0.3669022075699631\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(93.2950)\n",
            "total loss: 0.23082585747975276\n",
            "triplet loss: tensor(0.0711, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1090, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.7176)\n",
            "total loss: 0.3740222653341011\n",
            "triplet loss: 0.06919903907726502\n",
            "cross entropy loss: 0.3636424099728906\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(94.4067)\n",
            "total loss: 0.19334461290174837\n",
            "triplet loss: tensor(0.0588, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3279, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.5463)\n",
            "total loss: 0.3416606444843422\n",
            "triplet loss: 0.05176144255292133\n",
            "cross entropy loss: 0.33389642803803\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(95.2433)\n",
            "total loss: 0.16720349429401635\n",
            "triplet loss: tensor(0.0860, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2077, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.0185)\n",
            "total loss: 0.34390290535589646\n",
            "triplet loss: 0.09146619154989014\n",
            "cross entropy loss: 0.33018297622718756\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(95.8200)\n",
            "total loss: 0.1477613293154757\n",
            "triplet loss: tensor(0.0980, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2089, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.9028)\n",
            "total loss: 0.3448535102535282\n",
            "triplet loss: 0.07767903413513355\n",
            "cross entropy loss: 0.33320165436560584\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(96.2283)\n",
            "total loss: 0.13501302287506778\n",
            "triplet loss: tensor(0.0868, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3195, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.6389)\n",
            "total loss: 0.3443306891258652\n",
            "triplet loss: 0.055656190840362094\n",
            "cross entropy loss: 0.33598226077369686\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(96.6567)\n",
            "total loss: 0.12045182360769081\n",
            "triplet loss: tensor(0.0881, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2392, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.6620)\n",
            "total loss: 0.34906051404906446\n",
            "triplet loss: 0.0629925642096432\n",
            "cross entropy loss: 0.3396116285722637\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(96.8600)\n",
            "total loss: 0.11149397175282494\n",
            "triplet loss: tensor(0.0859, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1994, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.7778)\n",
            "total loss: 0.3357562468602107\n",
            "triplet loss: 0.06140821994717657\n",
            "cross entropy loss: 0.32654501438229044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **After Handling Test Missing**"
      ],
      "metadata": {
        "id": "LQkgInRlfNri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.fc_img = nn.Linear(32 * 8 * 8, 256)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.fc_combined = nn.Linear(512, 256)# Concatenated feature and image size\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.fc_domain = nn.Linear(256, 5)\n",
        "        self.fc_combined2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x_img):\n",
        "        x_img = self.conv1(x_img)\n",
        "        x_img = self.conv2(x_img)\n",
        "        x_img = x_img.view(x_img.size(0), -1)  # Flatten the image\n",
        "        x_img = F.relu(self.fc_img(x_img))\n",
        "        x_img = self.dropout1(x_img)\n",
        "\n",
        "        # Concatenate features and image representations\n",
        "        x_combined = torch.cat((x_img, x_img), dim=1)\n",
        "\n",
        "        # Fully connected layers for prediction\n",
        "        x_combined = F.relu(self.fc_combined(x_combined))\n",
        "        x_combined = self.dropout2(x_combined)\n",
        "        x_combined2 = self.fc_combined2(x_combined)\n",
        "        x_combined = self.fc_domain(x_combined)\n",
        "        return x_combined2, x_combined\n",
        "\n",
        "model = CombinedModel()\n",
        "crossLoss = nn.CrossEntropyLoss()\n",
        "triplet = triplet_loss(margin=0.008)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  print('epoch: ' + str(epoch))\n",
        "  model.train()\n",
        "  running_corrects = 0\n",
        "  running_loss = 0.0\n",
        "  triplet_total = 0.0\n",
        "  entropy_total = 0.0\n",
        "\n",
        "  num_samples = len(train.dataset)\n",
        "  num_batches = len(train)\n",
        "  for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(train):\n",
        "\n",
        "    outs2, outs = model(images)\n",
        "\n",
        "    loss1 = crossLoss(outs2, digit_labels)\n",
        "    loss2 = triplet(outs, domain_labels)\n",
        "    total_loss = loss1 + 0.4 * loss2\n",
        "\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    _, preds = torch.max(outs2, dim=1) # Explain, [N]\n",
        "    running_corrects += torch.sum(preds == digit_labels)\n",
        "    running_loss += total_loss.item()\n",
        "    triplet_total += loss2.item()\n",
        "    entropy_total += loss1.item()\n",
        "\n",
        "  epoch_acc = (running_corrects / num_samples) * 100\n",
        "  epoch_loss = (running_loss / num_batches)\n",
        "  triplet_total  = (triplet_total / num_batches)\n",
        "  entropy_total = (entropy_total / num_batches)\n",
        "  print('train')\n",
        "  print('accuaray: ' + str(epoch_acc))\n",
        "  print('total loss: ' + str(epoch_loss))\n",
        "  print('triplet loss: ' + str(loss2))\n",
        "  print('cross entropy loss: ' + str(loss1))\n",
        "  print('---------------')\n",
        "\n",
        "\n",
        "  num_samples_test = len(test.dataset)\n",
        "  num_batches_test = len(test)\n",
        "  running_corrects_test = 0\n",
        "  running_loss_test = 0.0\n",
        "  triplet_total = 0.0\n",
        "  entropy_total = 0.0\n",
        "\n",
        "  model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "  with torch.no_grad(): # explain\n",
        "      # more on torch.no_grad(): https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#disabling-gradient-tracking\n",
        "\n",
        "    for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(missing): # Get a batch of Data\n",
        "\n",
        "        outputs2, outputs = model(images) # Forward Pass\n",
        "        loss = crossLoss(outputs2, digit_labels) # Compute Loss\n",
        "        loss2 = triplet(outputs, domain_labels)\n",
        "        total_loss = loss + 0.4 * loss2\n",
        "\n",
        "        _, preds = torch.max(outputs2, 1) #\n",
        "        running_corrects_test += torch.sum(preds == digit_labels)\n",
        "        running_loss_test += total_loss.item()\n",
        "        triplet_total += loss2.item()\n",
        "        entropy_total += loss.item()\n",
        "\n",
        "    test_acc = (running_corrects_test / num_samples_test) * 100\n",
        "    test_loss = (running_loss_test / num_batches_test)\n",
        "    triplet_total = (triplet_total / num_batches_test)\n",
        "    entropy_total = (entropy_total / num_batches_test)\n",
        "    print('test')\n",
        "    print('accuray: ' + str(test_acc))\n",
        "    print('total loss: ' + str(test_loss))\n",
        "    print('triplet loss: ' + str(triplet_total))\n",
        "    print('cross entropy loss: ' + str(entropy_total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AALtFlvWYl2m",
        "outputId": "324b398d-64e8-4693-a36c-60a58895e17e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "train\n",
            "accuaray: tensor(75.9450)\n",
            "total loss: 0.7583446787999891\n",
            "triplet loss: tensor(0.0731, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.5737, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(89.2037)\n",
            "total loss: 0.3713491923622126\n",
            "triplet loss: 0.08576447091247202\n",
            "cross entropy loss: 0.35848452119961294\n",
            "epoch: 1\n",
            "train\n",
            "accuaray: tensor(88.7383)\n",
            "total loss: 0.37672739313927284\n",
            "triplet loss: tensor(0.0624, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3287, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(90.9815)\n",
            "total loss: 0.3123972717253767\n",
            "triplet loss: 0.07513624558655116\n",
            "cross entropy loss: 0.30112683444598015\n",
            "epoch: 2\n",
            "train\n",
            "accuaray: tensor(91.0667)\n",
            "total loss: 0.3002759632922566\n",
            "triplet loss: tensor(0.0618, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.3938, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(91.9537)\n",
            "total loss: 0.2833544951836033\n",
            "triplet loss: 0.08107345604155897\n",
            "cross entropy loss: 0.27119347652607767\n",
            "epoch: 3\n",
            "train\n",
            "accuaray: tensor(92.5133)\n",
            "total loss: 0.2516472511835444\n",
            "triplet loss: tensor(0.0581, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2643, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.1296)\n",
            "total loss: 0.27513358301135915\n",
            "triplet loss: 0.08982437113906153\n",
            "cross entropy loss: 0.2616599269167206\n",
            "epoch: 4\n",
            "train\n",
            "accuaray: tensor(93.4817)\n",
            "total loss: 0.2189294452161423\n",
            "triplet loss: tensor(0.0536, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2323, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.4769)\n",
            "total loss: 0.26424351654197337\n",
            "triplet loss: 0.056617118605583376\n",
            "cross entropy loss: 0.25575094813337695\n",
            "epoch: 5\n",
            "train\n",
            "accuaray: tensor(94.1483)\n",
            "total loss: 0.1970930162555119\n",
            "triplet loss: tensor(0.0678, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2050, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.3935)\n",
            "total loss: 0.2677328660092234\n",
            "triplet loss: 0.08599456260807416\n",
            "cross entropy loss: 0.25483368061615164\n",
            "epoch: 6\n",
            "train\n",
            "accuaray: tensor(94.8450)\n",
            "total loss: 0.17588703126620764\n",
            "triplet loss: tensor(0.0765, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1302, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.5093)\n",
            "total loss: 0.2767096499984081\n",
            "triplet loss: 0.08973043674283479\n",
            "cross entropy loss: 0.26325008326028226\n",
            "epoch: 7\n",
            "train\n",
            "accuaray: tensor(95.2550)\n",
            "total loss: 0.16409059858390454\n",
            "triplet loss: tensor(0.0836, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.1838, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.6250)\n",
            "total loss: 0.2651035154690228\n",
            "triplet loss: 0.07389628878359258\n",
            "cross entropy loss: 0.25401907214156444\n",
            "epoch: 8\n",
            "train\n",
            "accuaray: tensor(95.6533)\n",
            "total loss: 0.14957733135392418\n",
            "triplet loss: tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.0079, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(93.1481)\n",
            "total loss: 0.26101278585310167\n",
            "triplet loss: 0.0827656706806118\n",
            "cross entropy loss: 0.24859793470985087\n",
            "epoch: 9\n",
            "train\n",
            "accuaray: tensor(95.9817)\n",
            "total loss: 0.14059531487731028\n",
            "triplet loss: tensor(0.0905, grad_fn=<MeanBackward0>)\n",
            "cross entropy loss: tensor(0.2047, grad_fn=<NllLossBackward0>)\n",
            "---------------\n",
            "test\n",
            "accuray: tensor(92.7176)\n",
            "total loss: 0.27851684281833955\n",
            "triplet loss: 0.09165670009627497\n",
            "cross entropy loss: 0.2647683370208158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples_test = len(test.dataset)\n",
        "num_batches_test = len(test)\n",
        "running_corrects_test = 0\n",
        "running_loss_test = 0.0\n",
        "triplet_total = 0.0\n",
        "entropy_total = 0.0\n",
        "\n",
        "model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "with torch.no_grad(): # explain\n",
        "    # more on torch.no_grad(): https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#disabling-gradient-tracking\n",
        "\n",
        "  for batch_indx, (images, features, domain_labels, digit_labels) in enumerate(missing): # Get a batch of Data\n",
        "\n",
        "      outputs2, outputs = model(images) # Forward Pass\n",
        "      loss = crossLoss(outputs2, digit_labels) # Compute Loss\n",
        "      loss2 = triplet(outputs, domain_labels)\n",
        "      total_loss = loss + 0.4 * loss2\n",
        "\n",
        "      _, preds = torch.max(outputs2, 1) #\n",
        "      running_corrects_test += torch.sum(preds == digit_labels)\n",
        "      running_loss_test += total_loss.item()\n",
        "      triplet_total += loss2.item()\n",
        "      entropy_total += loss.item()\n",
        "  test_acc = (running_corrects_test / num_samples_test) * 100\n",
        "  test_loss = (running_loss_test / num_batches_test)\n",
        "  triplet_total = (triplet_total / num_batches_test)\n",
        "  entropy_total = (entropy_total / num_batches_test)\n",
        "  print('test')\n",
        "  print('accuray: ' + str(test_acc))\n",
        "  print('total loss: ' + str(test_loss))\n",
        "  print('triplet loss: ' + str(triplet_total))\n",
        "  print('cross entropy loss: ' + str(entropy_total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67-H_MSOdf8K",
        "outputId": "40333bcd-3842-4dc3-9856-3b9c96b5d71f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test\n",
            "accuray: tensor(92.7176)\n",
            "total loss: 0.2777532266849685\n",
            "triplet loss: 0.09169089670717363\n",
            "cross entropy loss: 0.2639995914161999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Manual Test On MNIST Image**"
      ],
      "metadata": {
        "id": "D8JfAj6kn6pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision.transforms import transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "transformer = transforms.Compose([\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
        "                                  ])\n",
        "\n",
        "im1 = Image.open(\"9944.jpg\")\n",
        "newsize = (32, 32)\n",
        "im1 = im1.resize(newsize)\n",
        "# Shows the image in image viewer\n",
        "# im1.show()\n",
        "rgb_image = np.expand_dims(im1, axis=2)\n",
        "rgb_image = np.repeat(rgb_image, 3, axis=2)\n",
        "im1 = Image.fromarray(rgb_image)\n",
        "im1.show()\n",
        "image_tensor = transformer(im1)\n",
        "image_tensor = image_tensor.unsqueeze_(0) #so img is not treated as a batch\n",
        "input_img = Variable(image_tensor)\n",
        "output, _ = model(input_img)\n",
        "print(output)\n",
        "print(torch.max(output, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "bSQRTGDpVyXm",
        "outputId": "8a51dd86-88ef-4941-8b1f-2ec831ce69ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F137DA4D7E0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -9.0794,  -8.2172,  -3.3046,   9.0099, -12.2262,  -0.6324,  -6.2912,\n",
            "          -4.7953,  -1.8685,  -2.9095]], grad_fn=<AddmmBackward0>)\n",
            "torch.return_types.max(\n",
            "values=tensor([9.0099], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([3]))\n"
          ]
        }
      ]
    }
  ]
}